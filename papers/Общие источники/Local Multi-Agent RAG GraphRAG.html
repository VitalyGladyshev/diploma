<!doctype html><html lang="en"><head><title data-rh="true">Local Multi-Agent  RAG Superbot using GraphRAG, AutoGen, Ollama, and Chainlit. | by Karthik Rajan | AI Advances</title><meta data-rh="true" charset="utf-8"/><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=1"/><meta data-rh="true" name="theme-color" content="#000000"/><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"/><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"/><meta data-rh="true" property="al:ios:app_name" content="Medium"/><meta data-rh="true" property="al:ios:app_store_id" content="828256236"/><meta data-rh="true" property="al:android:package" content="com.medium.reader"/><meta data-rh="true" property="fb:app_id" content="542599432471018"/><meta data-rh="true" property="og:site_name" content="Medium"/><meta data-rh="true" property="og:type" content="article"/><meta data-rh="true" property="article:published_time" content="2024-08-08T23:05:49.790Z"/><meta data-rh="true" name="title" content="Local Multi-Agent  RAG Superbot using GraphRAG, AutoGen, Ollama, and Chainlit. | by Karthik Rajan | AI Advances"/><meta data-rh="true" property="og:title" content="Microsoft’s GraphRAG + AutoGen + Ollama + Chainlit = Fully Local &amp; Free Multi-Agent RAG Superbot"/><meta data-rh="true" property="al:android:url" content="medium://p/61ad3759f06f"/><meta data-rh="true" property="al:ios:url" content="medium://p/61ad3759f06f"/><meta data-rh="true" property="al:android:app_name" content="Medium"/><meta data-rh="true" name="description" content="By combining GraphRAG’s RAG capabilities with AutoGen’s conversational and task-oriented functions, you can create a superbot that efficiently handles detailed queries, multi-page report generation, and data analysis. The best part? This app runs locally on a consumer-grade PC or laptop with decent GPUs, ensuring data privacy."/><meta data-rh="true" property="og:description" content="This superbot app integrates GraphRAG with AutoGen agents, powered by local LLMs from Ollama, for free &amp; offline embedding &amp; inference."/><meta data-rh="true" property="og:url" content="https://ai.gopubby.com/microsofts-graphrag-autogen-ollama-chainlit-fully-local-free-multi-agent-rag-superbot-61ad3759f06f"/><meta data-rh="true" property="al:web:url" content="https://ai.gopubby.com/microsofts-graphrag-autogen-ollama-chainlit-fully-local-free-multi-agent-rag-superbot-61ad3759f06f"/><meta data-rh="true" property="og:image" content="https://miro.medium.com/v2/resize:fit:1200/1*2ePkbvke5ObCuJu-g2pzsg.png"/><meta data-rh="true" property="article:author" content="https://medium.com/@karthikrajanv"/><meta data-rh="true" name="author" content="Karthik Rajan, Ph.D"/><meta data-rh="true" name="robots" content="index,noarchive,follow,max-image-preview:large"/><meta data-rh="true" name="referrer" content="unsafe-url"/><meta data-rh="true" property="twitter:title" content="Microsoft’s GraphRAG + AutoGen + Ollama + Chainlit = Fully Local &amp; Free Multi-Agent RAG Superbot"/><meta data-rh="true" name="twitter:site" content="@AIAdvances"/><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/61ad3759f06f"/><meta data-rh="true" property="twitter:description" content="This superbot app integrates GraphRAG with AutoGen agents, powered by local LLMs from Ollama, for free &amp; offline embedding &amp; inference."/><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/v2/resize:fit:1200/1*2ePkbvke5ObCuJu-g2pzsg.png"/><meta data-rh="true" name="twitter:card" content="summary_large_image"/><meta data-rh="true" name="twitter:label1" content="Reading time"/><meta data-rh="true" name="twitter:data1" content="11 min read"/><link data-rh="true" rel="icon" href="https://miro.medium.com/v2/resize:fill:256:256/1*8xkFb5PlJ9-jlzImmpZsGg.png"/><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="/osd.xml"/><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://miro.medium.com/v2/resize:fill:304:304/10fd5c419ac61637245384e7099e131627900034828f4f386bdaa47a74eae156"/><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://miro.medium.com/v2/resize:fill:240:240/10fd5c419ac61637245384e7099e131627900034828f4f386bdaa47a74eae156"/><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://miro.medium.com/v2/resize:fill:152:152/10fd5c419ac61637245384e7099e131627900034828f4f386bdaa47a74eae156"/><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://miro.medium.com/v2/resize:fill:120:120/10fd5c419ac61637245384e7099e131627900034828f4f386bdaa47a74eae156"/><link data-rh="true" rel="mask-icon" href="https://miro.medium.com/v2/resize:fill:1000:1000/7*GAOKVe--MXbEJmV9230oOQ.png" color="#171717"/><link data-rh="true" id="glyph_preload_link" rel="preload" as="style" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" rel="author" href="https://medium.com/@karthikrajanv"/><link data-rh="true" rel="canonical" href="https://ai.gopubby.com/microsofts-graphrag-autogen-ollama-chainlit-fully-local-free-multi-agent-rag-superbot-61ad3759f06f"/><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/61ad3759f06f"/><script data-rh="true" type="application/ld+json">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fv2\u002Fresize:fit:1200\u002F1*2ePkbvke5ObCuJu-g2pzsg.png"],"url":"https:\u002F\u002Fai.gopubby.com\u002Fmicrosofts-graphrag-autogen-ollama-chainlit-fully-local-free-multi-agent-rag-superbot-61ad3759f06f","dateCreated":"2024-07-15T03:58:54.326Z","datePublished":"2024-07-15T03:58:54.326Z","dateModified":"2024-11-20T20:04:49.158Z","headline":"Local Multi-Agent  RAG Superbot using GraphRAG, AutoGen, Ollama, and Chainlit. | by Karthik Rajan | AI Advances","name":"Local Multi-Agent  RAG Superbot using GraphRAG, AutoGen, Ollama, and Chainlit. | by Karthik Rajan | AI Advances","description":"By combining GraphRAG’s RAG capabilities with AutoGen’s conversational and task-oriented functions, you can create a superbot that efficiently handles detailed queries, multi-page report generation, and data analysis. The best part? This app runs locally on a consumer-grade PC or laptop with decent GPUs, ensuring data privacy.","identifier":"61ad3759f06f","author":{"@type":"Person","name":"Karthik Rajan, Ph.D","url":"https:\u002F\u002Fai.gopubby.com\u002F@karthikrajanv"},"creator":["Karthik Rajan, Ph.D"],"publisher":{"@type":"Organization","name":"AI Advances","url":"ai.gopubby.com","logo":{"@type":"ImageObject","width":272,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fv2\u002Fresize:fit:544\u002F7*V1_7XP4snlmqrc_0Njontw.png"}},"mainEntityOfPage":"https:\u002F\u002Fai.gopubby.com\u002Fmicrosofts-graphrag-autogen-ollama-chainlit-fully-local-free-multi-agent-rag-superbot-61ad3759f06f"}</script><style type="text/css" data-fela-rehydration="598" data-fela-type="STATIC">html{box-sizing:border-box;-webkit-text-size-adjust:100%}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}#speechify-root{font-family:Sohne, sans-serif}div[data-popper-reference-hidden="true"]{visibility:hidden;pointer-events:none}.grecaptcha-badge{visibility:hidden}
/*XCode style (c) Angel Garcia <angelgarcia.mail@gmail.com>*/.hljs {background: #fff;color: black;
}/* Gray DOCTYPE selectors like WebKit */
.xml .hljs-meta {color: #c0c0c0;
}.hljs-comment,
.hljs-quote {color: #007400;
}.hljs-tag,
.hljs-attribute,
.hljs-keyword,
.hljs-selector-tag,
.hljs-literal,
.hljs-name {color: #aa0d91;
}.hljs-variable,
.hljs-template-variable {color: #3F6E74;
}.hljs-code,
.hljs-string,
.hljs-meta .hljs-string {color: #c41a16;
}.hljs-regexp,
.hljs-link {color: #0E0EFF;
}.hljs-title,
.hljs-symbol,
.hljs-bullet,
.hljs-number {color: #1c00cf;
}.hljs-section,
.hljs-meta {color: #643820;
}.hljs-title.class_,
.hljs-class .hljs-title,
.hljs-type,
.hljs-built_in,
.hljs-params {color: #5c2699;
}.hljs-attr {color: #836C28;
}.hljs-subst {color: #000;
}.hljs-formula {background-color: #eee;font-style: italic;
}.hljs-addition {background-color: #baeeba;
}.hljs-deletion {background-color: #ffc8bd;
}.hljs-selector-id,
.hljs-selector-class {color: #9b703f;
}.hljs-doctag,
.hljs-strong {font-weight: bold;
}.hljs-emphasis {font-style: italic;
}
</style><style type="text/css" data-fela-rehydration="598" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-moz-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-webkit-keyframes k2{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k2{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k2{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}</style><style type="text/css" data-fela-rehydration="598" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{display:block}.m{position:sticky}.n{top:0}.o{z-index:500}.p{padding:0 24px}.q{align-items:center}.r{border-bottom:solid 1px #F2F2F2}.y{height:41px}.z{line-height:20px}.ab{display:flex}.ac{height:57px}.ae{flex:1 0 auto}.af{color:inherit}.ag{fill:inherit}.ah{font-size:inherit}.ai{border:inherit}.aj{font-family:inherit}.ak{letter-spacing:inherit}.al{font-weight:inherit}.am{padding:0}.an{margin:0}.ao{cursor:pointer}.ap:disabled{cursor:not-allowed}.aq:disabled{color:#6B6B6B}.ar:disabled{fill:#6B6B6B}.au{width:auto}.av path{fill:#242424}.aw{height:25px}.ax{margin-left:16px}.ay{border:none}.az{border-radius:20px}.ba{width:240px}.bb{background:#F9F9F9}.bc path{fill:#6B6B6B}.be{outline:none}.bf{font-family:sohne, "Helvetica Neue", Helvetica, Arial, sans-serif}.bg{font-size:14px}.bh{width:100%}.bi{padding:10px 20px 10px 0}.bj{background-color:transparent}.bk{color:#242424}.bl::placeholder{color:#6B6B6B}.bm{display:inline-block}.bn{margin-left:12px}.bo{margin-right:12px}.bp{border-radius:4px}.bq{margin-left:24px}.br{height:24px}.bx{background-color:#F9F9F9}.by{border-radius:50%}.bz{height:32px}.ca{width:32px}.cb{justify-content:center}.ch{max-width:680px}.ci{min-width:0}.cj{animation:k1 1.2s ease-in-out infinite}.ck{height:100vh}.cl{margin-bottom:16px}.cm{margin-top:48px}.cn{align-items:flex-start}.co{flex-direction:column}.cp{justify-content:space-between}.cq{margin-bottom:24px}.cw{width:80%}.cx{background-color:#F2F2F2}.dd{height:44px}.de{width:44px}.df{margin:auto 0}.dg{margin-bottom:4px}.dh{height:16px}.di{width:120px}.dj{width:80px}.dp{margin-bottom:8px}.dq{width:96%}.dr{width:98%}.ds{width:81%}.dw{margin-left:8px}.dx{color:#6B6B6B}.dy{font-size:13px}.dz{height:100%}.ec{margin-right:32px}.ed{position:relative}.ee{fill:#6B6B6B}.eh{background:transparent}.ei svg{margin-left:4px}.ej svg{fill:#6B6B6B}.el{box-shadow:inset 0 0 0 1px rgba(0, 0, 0, 0.05)}.em{position:absolute}.ep{box-sizing:border-box}.ev{margin:0 24px}.ez{background:rgba(255, 255, 255, 1)}.fa{border:1px solid #F2F2F2}.fb{box-shadow:0 1px 4px #F2F2F2}.fc{max-height:100vh}.fd{overflow-y:auto}.fe{left:0}.ff{top:calc(100vh + 100px)}.fg{bottom:calc(100vh + 100px)}.fh{width:10px}.fi{pointer-events:none}.fj{word-break:break-word}.fk{word-wrap:break-word}.fl:after{display:block}.fm:after{content:""}.fn:after{clear:both}.fo{line-height:1.23}.fp{letter-spacing:0}.fq{font-style:normal}.fr{font-weight:700}.gm{margin-bottom:-0.27em}.gn{line-height:1.394}.hi{align-items:baseline}.hj{width:48px}.hk{height:48px}.hl{border:2px solid rgba(255, 255, 255, 1)}.hm{z-index:0}.hn{box-shadow:none}.ho{border:1px solid rgba(0, 0, 0, 0.05)}.hp{margin-left:-12px}.hq{width:28px}.hr{height:28px}.hs{z-index:1}.ht{width:24px}.hu{margin-bottom:2px}.hv{flex-wrap:nowrap}.hw{font-size:16px}.hx{line-height:24px}.hz{margin:0 8px}.ia{display:inline}.ib{color:rgba(98, 136, 193, 1)}.ic{fill:rgba(98, 136, 193, 1)}.id:disabled{opacity:0.3}.ig{flex:0 0 auto}.ij{flex-wrap:wrap}.ik{white-space:pre-wrap}.il{margin-right:4px}.im{overflow:hidden}.in{max-height:20px}.io{text-overflow:ellipsis}.ip{display:-webkit-box}.iq{-webkit-line-clamp:1}.ir{-webkit-box-orient:vertical}.is{word-break:break-all}.iu{padding-left:8px}.iv{padding-right:8px}.jw> *{flex-shrink:0}.jx{overflow-x:scroll}.jy::-webkit-scrollbar{display:none}.jz{scrollbar-width:none}.ka{-ms-overflow-style:none}.kb{width:74px}.kc{flex-direction:row}.kd{z-index:2}.kg{-webkit-user-select:none}.kh{border:0}.ki{cursor:progress}.kj{fill:rgba(117, 117, 117, 1)}.km{opacity:0.25}.kn{outline:0}.ko{user-select:none}.kp> svg{pointer-events:none}.ky{margin-left:4px}.kz{margin-top:0px}.la{opacity:1}.lb{padding:4px 0}.le{width:16px}.lf{padding:8px 2px}.li svg path{fill:#6B6B6B}.lj{display:inline-flex}.lp{max-width:100%}.lq svg{color:#6B6B6B}.mh{clear:both}.mn{margin-left:auto}.mo{margin-right:auto}.mp{max-width:3072px}.mv{padding-top:5px}.mw{padding-bottom:5px}.my{cursor:zoom-in}.mz{z-index:auto}.nb{height:auto}.nc{margin-top:10px}.nd{text-align:center}.ne{max-width:728px}.nh{line-height:1.58}.ni{letter-spacing:-0.004em}.nj{font-family:source-serif-pro, Georgia, Cambria, "Times New Roman", Times, serif}.oc{margin-bottom:-0.46em}.od{clear:left}.oe{float:left}.of{font-size:66px}.og{line-height:.83}.om{max-width:810px}.on{text-decoration:underline}.oo{box-shadow:inset 3px 0 0 0 #242424}.op{padding-left:23px}.oq{margin-left:-20px}.or{font-style:italic}.os{list-style-type:decimal}.ot{margin-left:30px}.ou{padding-left:0px}.pa{max-width:1512px}.pb{max-width:423px}.pc{margin-top:32px}.pd{margin-bottom:14px}.pe{padding-top:24px}.pf{padding-bottom:10px}.pg{background-color:#000000}.ph{height:3px}.pi{width:3px}.pj{margin-right:20px}.pk{line-height:1.12}.pl{letter-spacing:-0.022em}.pm{font-weight:600}.qf{margin-bottom:-0.28em}.qg{line-height:1.18}.qw{margin-bottom:-0.31em}.qx{overflow-x:auto}.qy{font-family:source-code-pro, Menlo, Monaco, "Courier New", Courier, monospace}.qz{padding:32px}.ra{border:1px solid #E5E5E5}.rb{line-height:1.4}.rc{margin-top:-0.2em}.rd{margin-bottom:-0.2em}.re{white-space:pre}.rf{min-width:fit-content}.rg{padding:2px 4px}.rh{font-size:75%}.ri> strong{font-family:inherit}.rt{margin-bottom:26px}.ru{margin-top:6px}.rv{margin-top:8px}.rw{margin-right:8px}.rx{padding:8px 16px}.ry{border-radius:100px}.rz{transition:background 300ms ease}.sb{white-space:nowrap}.sc{border-top:none}.sd{margin-bottom:50px}.se{height:52px}.sf{max-height:52px}.sg{box-sizing:content-box}.sh{position:static}.sj{max-width:155px}.sp{margin-bottom:64px}.sq{margin-bottom:48px}.te{border-radius:2px}.tg{height:64px}.th{width:64px}.ti{align-self:flex-end}.tj{color:#F2F2F2}.tk{fill:#F2F2F2}.tl{background:#F2F2F2}.tm{border-color:#F2F2F2}.ts:disabled{cursor:inherit !important}.tt:disabled{opacity:0.1}.tu:disabled:hover{background:rgba(25, 25, 25, 1)}.tv:disabled:hover{border-color:rgba(25, 25, 25, 1)}.tw{border-radius:99em}.tx{border-width:1px}.ty{border-style:solid}.tz{text-decoration:none}.ua{flex:1 1 auto}.ue{padding-right:4px}.uf{font-weight:500}.um{margin-top:16px}.us{height:0px}.ut{gap:18px}.uu{fill:rgba(61, 61, 61, 1)}.uw{padding-bottom:6px}.ux{border-bottom:1px solid #F2F2F2}.vd{fill:#242424}.ve{background:0}.vf{border-color:#242424}.vj:disabled:hover{color:#242424}.vk:disabled:hover{fill:#242424}.vl:disabled:hover{border-color:#242424}.vw{border-bottom:solid 1px #E5E5E5}.vx{margin-top:72px}.vy{padding:24px 0}.vz{margin-bottom:0px}.wa{margin-right:16px}.as:hover:not(:disabled){color:rgba(25, 25, 25, 1)}.at:hover:not(:disabled){fill:rgba(25, 25, 25, 1)}.ef:hover{color:#242424}.eg:hover{fill:#242424}.ek:hover svg{fill:#242424}.eo:hover{background-color:rgba(0, 0, 0, 0.1)}.hy:hover{text-decoration:underline}.ie:hover:not(:disabled){color:rgba(87, 117, 162, 1)}.if:hover:not(:disabled){fill:rgba(87, 117, 162, 1)}.kl:hover{fill:rgba(117, 117, 117, 1)}.lc:hover{fill:#000000}.ld:hover p{color:#000000}.lg:hover:not(:disabled) svg path{fill:#000000}.lr:hover svg{color:#000000}.sa:hover{background-color:#F2F2F2}.tf:hover{background-color:none}.tn:hover{background:#F2F2F2}.to:hover{border-color:#F2F2F2}.tp:hover{cursor:wait}.tq:hover{color:#F2F2F2}.tr:hover{fill:#F2F2F2}.uv:hover{fill:rgba(25, 25, 25, 1)}.vg:hover{color:#000000}.vh:hover{border-color:#242424}.vi:hover{cursor:pointer}.bd:focus-within path{fill:#242424}.kk:focus{fill:rgba(117, 117, 117, 1)}.lh:focus svg path{fill:#000000}.ls:focus svg{color:#000000}.na:focus{transform:scale(1.01)}.kq:active{border-style:none}</style><style type="text/css" data-fela-rehydration="598" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.bw{width:64px}.cg{margin:0 64px}.cv{height:48px}.dc{margin-bottom:52px}.do{margin-bottom:48px}.eb{display:flex}.eu{margin-bottom:50px}.ey{max-width:680px}.gi{font-size:42px}.gj{margin-top:1.19em}.gk{line-height:52px}.gl{letter-spacing:-0.011em}.ha{font-size:22px}.hb{margin-top:0.92em}.hc{line-height:28px}.hh{align-items:center}.ji{border-top:solid 1px #F2F2F2}.jj{border-bottom:solid 1px #F2F2F2}.jk{margin:32px 0 0}.jl{padding:3px 8px}.ju> *{margin-right:24px}.jv> :last-child{margin-right:0}.kx{margin-top:0px}.lo{margin:0}.mm{max-width:1192px}.mu{margin-top:56px}.ny{font-size:20px}.nz{margin-top:2.14em}.oa{line-height:32px}.ob{letter-spacing:-0.003em}.ol{padding-top:7px}.oz{margin-top:1.14em}.qb{font-size:24px}.qc{margin-top:1.25em}.qd{line-height:30px}.qe{letter-spacing:-0.016em}.qt{margin-top:1.72em}.qu{line-height:24px}.qv{letter-spacing:0}.rn{margin-top:1.95em}.rs{margin-top:0.94em}.so{display:inline-block}.sr{flex-direction:row}.su{margin-bottom:0}.sv{margin-right:20px}.ub{max-width:500px}.ur{margin-bottom:88px}.vc{margin:40px 0 16px}.vq{width:min-width}.vv{padding-top:72px}</style><style type="text/css" data-fela-rehydration="598" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.kw{margin-top:0px}.nf{margin-left:auto}.ng{text-align:center}.sn{display:inline-block}</style><style type="text/css" data-fela-rehydration="598" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.kv{margin-top:0px}.sm{display:inline-block}</style><style type="text/css" data-fela-rehydration="598" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.kt{margin-top:0px}.ku{margin-right:0px}.sl{display:inline-block}</style><style type="text/css" data-fela-rehydration="598" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.s{display:flex}.t{justify-content:space-between}.bs{width:24px}.cc{margin:0 24px}.cr{height:40px}.cy{margin-bottom:44px}.dk{margin-bottom:32px}.dt{justify-content:center}.eq{margin-bottom:2px}.fs{font-size:32px}.ft{margin-top:1.01em}.fu{line-height:38px}.fv{letter-spacing:-0.014em}.go{font-size:18px}.gp{margin-top:0.79em}.gq{line-height:24px}.hd{align-items:flex-start}.ih{flex-direction:column}.iw{margin:24px -24px 0}.ix{padding:0}.jm> *{margin-right:8px}.jn> :last-child{margin-right:24px}.ke{margin-left:0px}.kr{margin-top:0px}.ks{margin-right:0px}.lk{margin:0}.lt{border:1px solid #F2F2F2}.lu{border-radius:99em}.lv{padding:0px 16px 0px 12px}.lw{height:38px}.lx{align-items:center}.lz svg{margin-right:8px}.mi{max-width:100%}.mq{margin-top:40px}.nk{margin-top:1.56em}.nl{line-height:28px}.nm{letter-spacing:-0.003em}.oh{padding-top:0}.ov{margin-top:1.34em}.pn{font-size:20px}.po{margin-top:0.93em}.pp{letter-spacing:0}.qh{font-size:16px}.qi{margin-top:1.23em}.qj{line-height:20px}.rj{margin-top:1.2em}.ro{margin-top:0.67em}.sk{display:inline-block}.tc{margin-bottom:20px}.td{margin-right:0}.ug{font-size:24px}.uh{line-height:30px}.ui{letter-spacing:-0.016em}.un{margin-bottom:64px}.uy{margin:32px 0 16px}.vm{width:100%}.vr{padding-top:48px}.ly:hover{border-color:#E5E5E5}</style><style type="text/css" data-fela-rehydration="598" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.bv{width:64px}.cf{margin:0 64px}.cu{height:48px}.db{margin-bottom:52px}.dn{margin-bottom:48px}.ea{display:flex}.et{margin-bottom:50px}.ex{max-width:680px}.ge{font-size:42px}.gf{margin-top:1.19em}.gg{line-height:52px}.gh{letter-spacing:-0.011em}.gx{font-size:22px}.gy{margin-top:0.92em}.gz{line-height:28px}.hg{align-items:center}.je{border-top:solid 1px #F2F2F2}.jf{border-bottom:solid 1px #F2F2F2}.jg{margin:32px 0 0}.jh{padding:3px 8px}.js> *{margin-right:24px}.jt> :last-child{margin-right:0}.ln{margin:0}.ml{max-width:1192px}.mt{margin-top:56px}.nu{font-size:20px}.nv{margin-top:2.14em}.nw{line-height:32px}.nx{letter-spacing:-0.003em}.ok{padding-top:7px}.oy{margin-top:1.14em}.px{font-size:24px}.py{margin-top:1.25em}.pz{line-height:30px}.qa{letter-spacing:-0.016em}.qq{margin-top:1.72em}.qr{line-height:24px}.qs{letter-spacing:0}.rm{margin-top:1.95em}.rr{margin-top:0.94em}.ss{flex-direction:row}.sw{margin-bottom:0}.sx{margin-right:20px}.uc{max-width:500px}.uq{margin-bottom:88px}.vb{margin:40px 0 16px}.vp{width:min-width}.vu{padding-top:72px}</style><style type="text/css" data-fela-rehydration="598" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.w{display:flex}.x{justify-content:space-between}.bu{width:64px}.ce{margin:0 48px}.ct{height:48px}.da{margin-bottom:52px}.dm{margin-bottom:48px}.dv{justify-content:center}.es{margin-bottom:50px}.ew{max-width:680px}.ga{font-size:42px}.gb{margin-top:1.19em}.gc{line-height:52px}.gd{letter-spacing:-0.011em}.gu{font-size:22px}.gv{margin-top:0.92em}.gw{line-height:28px}.hf{align-items:center}.ja{border-top:solid 1px #F2F2F2}.jb{border-bottom:solid 1px #F2F2F2}.jc{margin:32px 0 0}.jd{padding:3px 8px}.jq> *{margin-right:24px}.jr> :last-child{margin-right:0}.lm{margin:0}.mk{max-width:100%}.ms{margin-top:56px}.nq{font-size:20px}.nr{margin-top:2.14em}.ns{line-height:32px}.nt{letter-spacing:-0.003em}.oj{padding-top:7px}.ox{margin-top:1.14em}.pt{font-size:24px}.pu{margin-top:1.25em}.pv{line-height:30px}.pw{letter-spacing:-0.016em}.qn{margin-top:1.72em}.qo{line-height:24px}.qp{letter-spacing:0}.rl{margin-top:1.95em}.rq{margin-top:0.94em}.st{flex-direction:row}.sy{margin-bottom:0}.sz{margin-right:20px}.ud{max-width:500px}.up{margin-bottom:88px}.va{margin:40px 0 16px}.vo{width:min-width}.vt{padding-top:72px}</style><style type="text/css" data-fela-rehydration="598" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.u{display:flex}.v{justify-content:space-between}.bt{width:24px}.cd{margin:0 24px}.cs{height:40px}.cz{margin-bottom:44px}.dl{margin-bottom:32px}.du{justify-content:center}.er{margin-bottom:2px}.fw{font-size:32px}.fx{margin-top:1.01em}.fy{line-height:38px}.fz{letter-spacing:-0.014em}.gr{font-size:18px}.gs{margin-top:0.79em}.gt{line-height:24px}.he{align-items:flex-start}.ii{flex-direction:column}.iy{margin:24px 0 0}.iz{padding:0}.jo> *{margin-right:8px}.jp> :last-child{margin-right:8px}.kf{margin-left:0px}.ll{margin:0}.ma{border:1px solid #F2F2F2}.mb{border-radius:99em}.mc{padding:0px 16px 0px 12px}.md{height:38px}.me{align-items:center}.mg svg{margin-right:8px}.mj{max-width:100%}.mr{margin-top:40px}.nn{margin-top:1.56em}.no{line-height:28px}.np{letter-spacing:-0.003em}.oi{padding-top:0}.ow{margin-top:1.34em}.pq{font-size:20px}.pr{margin-top:0.93em}.ps{letter-spacing:0}.qk{font-size:16px}.ql{margin-top:1.23em}.qm{line-height:20px}.rk{margin-top:1.2em}.rp{margin-top:0.67em}.ta{margin-bottom:20px}.tb{margin-right:0}.uj{font-size:24px}.uk{line-height:30px}.ul{letter-spacing:-0.016em}.uo{margin-bottom:64px}.uz{margin:32px 0 16px}.vn{width:100%}.vs{padding-top:48px}.mf:hover{border-color:#E5E5E5}</style><style type="text/css" data-fela-rehydration="598" data-fela-type="RULE" media="print">.si{display:none}</style><style type="text/css" data-fela-rehydration="598" data-fela-type="RULE" media="(orientation: landscape) and (max-width: 903.98px)">.it{max-height:none}</style><style type="text/css" data-fela-rehydration="598" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.mx{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}</style></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><div class="l c"><div class="l m n o c"><div class="am q r s dt u du w dv i d y z"><a class="dx ag dy bf ak b am an ao ap aq ar as at s u w i d q dz z" href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F61ad3759f06f&amp;%7Efeature=LiOpenInAppButton&amp;%7Echannel=ShowPostUnderCollection&amp;source=---top_nav_layout_nav-----------------------------------------" rel="noopener follow">Open in app<svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" fill="none" viewBox="0 0 10 10" class="dw"><path fill="currentColor" d="M.985 8.485a.375.375 0 1 0 .53.53zM8.75 1.25h.375A.375.375 0 0 0 8.75.875zM8.375 6.5a.375.375 0 1 0 .75 0zM3.5.875a.375.375 0 1 0 0 .75zm-1.985 8.14 7.5-7.5-.53-.53-7.5 7.5zm6.86-7.765V6.5h.75V1.25zM3.5 1.625h5.25v-.75H3.5z"></path></svg></a></div><div class="p q r ab ac"><div class="ab q ae"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab" aria-label="Homepage" data-testid="headerMediumLogo" href="https://medium.com/?source=---top_nav_layout_nav-----------------------------------------" rel="noopener follow"><svg xmlns="http://www.w3.org/2000/svg" width="719" height="160" fill="none" viewBox="0 0 719 160" class="au av aw"><path fill="#242424" d="m174.104 9.734.215-.047V8.02H130.39L89.6 103.89 48.81 8.021H1.472v1.666l.212.047c8.018 1.81 12.09 4.509 12.09 14.242V137.93c0 9.734-4.087 12.433-12.106 14.243l-.212.047v1.671h32.118v-1.665l-.213-.048c-8.018-1.809-12.089-4.509-12.089-14.242V30.586l52.399 123.305h2.972l53.925-126.743V140.75c-.687 7.688-4.721 10.062-11.982 11.701l-.215.05v1.652h55.948v-1.652l-.215-.05c-7.269-1.639-11.4-4.013-12.087-11.701l-.037-116.774h.037c0-9.733 4.071-12.432 12.087-14.242m25.555 75.488c.915-20.474 8.268-35.252 20.606-35.507 3.806.063 6.998 1.312 9.479 3.714 5.272 5.118 7.751 15.812 7.368 31.793zm-.553 5.77h65.573v-.275c-.186-15.656-4.721-27.834-13.466-36.196-7.559-7.227-18.751-11.203-30.507-11.203h-.263c-6.101 0-13.584 1.48-18.909 4.16-6.061 2.807-11.407 7.003-15.855 12.511-7.161 8.874-11.499 20.866-12.554 34.343q-.05.606-.092 1.212a50 50 0 0 0-.065 1.151 85.807 85.807 0 0 0-.094 5.689c.71 30.524 17.198 54.917 46.483 54.917 25.705 0 40.675-18.791 44.407-44.013l-1.886-.664c-6.557 13.556-18.334 21.771-31.738 20.769-18.297-1.369-32.314-19.922-31.042-42.395m139.722 41.359c-2.151 5.101-6.639 7.908-12.653 7.908s-11.513-4.129-15.418-11.63c-4.197-8.053-6.405-19.436-6.405-32.92 0-28.067 8.729-46.22 22.24-46.22 5.657 0 10.111 2.807 12.236 7.704zm43.499 20.008c-8.019-1.897-12.089-4.722-12.089-14.951V1.309l-48.716 14.353v1.757l.299-.024c6.72-.543 11.278.386 13.925 2.83 2.072 1.915 3.082 4.853 3.082 8.987v18.66c-4.803-3.067-10.516-4.56-17.448-4.56-14.059 0-26.909 5.92-36.176 16.672-9.66 11.205-14.767 26.518-14.767 44.278-.003 31.72 15.612 53.039 38.851 53.039 13.595 0 24.533-7.449 29.54-20.013v16.865h43.711v-1.746zM424.1 19.819c0-9.904-7.468-17.374-17.375-17.374-9.859 0-17.573 7.632-17.573 17.374s7.721 17.374 17.573 17.374c9.907 0 17.375-7.47 17.375-17.374m11.499 132.546c-8.019-1.897-12.089-4.722-12.089-14.951h-.035V43.635l-43.714 12.551v1.705l.263.024c9.458.842 12.047 4.1 12.047 15.152v81.086h43.751v-1.746zm112.013 0c-8.018-1.897-12.089-4.722-12.089-14.951V43.635l-41.621 12.137v1.71l.246.026c7.733.813 9.967 4.257 9.967 15.36v59.279c-2.578 5.102-7.415 8.131-13.274 8.336-9.503 0-14.736-6.419-14.736-18.073V43.638l-43.714 12.55v1.703l.262.024c9.459.84 12.05 4.097 12.05 15.152v50.17a56.3 56.3 0 0 0 .91 10.444l.787 3.423c3.701 13.262 13.398 20.197 28.59 20.197 12.868 0 24.147-7.966 29.115-20.43v17.311h43.714v-1.747zm169.818 1.788v-1.749l-.213-.05c-8.7-2.006-12.089-5.789-12.089-13.49v-63.79c0-19.89-11.171-31.761-29.883-31.761-13.64 0-25.141 7.882-29.569 20.16-3.517-13.01-13.639-20.16-28.606-20.16-13.146 0-23.449 6.938-27.869 18.657V43.643L545.487 55.68v1.715l.263.024c9.345.829 12.047 4.181 12.047 14.95v81.784h40.787v-1.746l-.215-.053c-6.941-1.631-9.181-4.606-9.181-12.239V66.998c1.836-4.289 5.537-9.37 12.853-9.37 9.086 0 13.692 6.296 13.692 18.697v77.828h40.797v-1.746l-.215-.053c-6.94-1.631-9.18-4.606-9.18-12.239V75.066a42 42 0 0 0-.578-7.26c1.947-4.661 5.86-10.177 13.475-10.177 9.214 0 13.691 6.114 13.691 18.696v77.828z"></path></svg></a><div class="ax h"><div class="ab ay az ba bb q bc bd"><div class="bm" aria-hidden="false" aria-describedby="searchResults" aria-labelledby="searchResults"></div><div class="bn bo ab"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.092 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0m6.95-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .79-.79l-3.73-3.73A8.05 8.05 0 0 0 11.042 3z" clip-rule="evenodd"></path></svg></div><input role="combobox" aria-controls="searchResults" aria-expanded="false" aria-label="search" data-testid="headerSearchInput" tabindex="0" class="ay be bf bg z bh bi bj bk bl" placeholder="Search" value=""/></div></div></div><div class="h k w ea eb"><div class="ec ab"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerWriteButton" href="https://medium.com/new-story?source=---top_nav_layout_nav-----------------------------------------" rel="noopener follow"><div class="bf b bg z dx ed ee ab q ef eg"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" aria-label="Write"><path fill="currentColor" d="M14 4a.5.5 0 0 0 0-1zm7 6a.5.5 0 0 0-1 0zm-7-7H4v1h10zM3 4v16h1V4zm1 17h16v-1H4zm17-1V10h-1v10zm-1 1a1 1 0 0 0 1-1h-1zM3 20a1 1 0 0 0 1 1v-1zM4 3a1 1 0 0 0-1 1h1z"></path><path stroke="currentColor" d="m17.5 4.5-8.458 8.458a.25.25 0 0 0-.06.098l-.824 2.47a.25.25 0 0 0 .316.316l2.47-.823a.25.25 0 0 0 .098-.06L19.5 6.5m-2-2 2.323-2.323a.25.25 0 0 1 .354 0l1.646 1.646a.25.25 0 0 1 0 .354L19.5 6.5m-2-2 2 2"></path></svg><div class="dw l">Write</div></div></a></div></div><div class="k j i d"><div class="ec ab"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerSearchButton" href="https://medium.com/search?source=---top_nav_layout_nav-----------------------------------------" rel="noopener follow"><div class="bf b bg z dx ed ee ab q ef eg"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" aria-label="Search"><path fill="currentColor" fill-rule="evenodd" d="M4.092 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0m6.95-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .79-.79l-3.73-3.73A8.05 8.05 0 0 0 11.042 3z" clip-rule="evenodd"></path></svg></div></a></div></div><div class="ec ab"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerNotificationButton" href="https://medium.com/me/notifications?source=---top_nav_layout_nav-----------------------------------------" rel="noopener follow"><div class="bf b bg z dx ed ee ab q ef eg"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" aria-label="Notifications"><path stroke="currentColor" stroke-linecap="round" d="M15 18.5a3 3 0 1 1-6 0"></path><path stroke="currentColor" stroke-linejoin="round" d="M5.5 10.532V9a6.5 6.5 0 0 1 13 0v1.532c0 1.42.564 2.782 1.568 3.786l.032.032c.256.256.4.604.4.966v2.934a.25.25 0 0 1-.25.25H3.75a.25.25 0 0 1-.25-.25v-2.934c0-.363.144-.71.4-.966l.032-.032A5.35 5.35 0 0 0 5.5 10.532Z"></path></svg></div></a></div><div class="l" aria-hidden="false"><button class="ay eh am ab q ao ei ej ek" aria-label="user options menu" data-testid="headerUserIcon"><div class="l ed"><div class="l ed"><img alt="Виталий" class="l ep by bz ca cx" src="https://miro.medium.com/v2/resize:fill:64:64/0*L8-gaiBv3nAhl_9Y.jpg" width="32" height="32" loading="lazy"/><div class="el by l bz ca em n ay eo"></div></div></div></button></div></div></div><div class="l"><div class="eq er es et eu l"><div class="ab cb"><div class="ci bh ev ew ex ey"></div></div><article><div class="l"><div class="l"><span class="l"></span><section><div><div class="em fe ff fg fh fi"></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div><h1 id="bbf2" class="pw-post-title fo fp fq bf fr fs ft fu fv fw fx fy fz ga gb gc gd ge gf gg gh gi gj gk gl gm bk" data-testid="storyTitle">Microsoft’s GraphRAG + AutoGen + Ollama + Chainlit = Local &amp; Free Multi-Agent RAG Superbot</h1></div><div><h2 id="5af4" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Guide to constructing an agentic-GraphRAG retrieval application—all codes in my GitHub repo.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@karthikrajanv?source=post_page---byline--61ad3759f06f---------------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Karthik Rajan, Ph.D" class="l ep by dd de cx" src="https://miro.medium.com/v2/resize:fill:88:88/1*mGw0B5wtYwJDrrVhxaL2NA.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"/><div class="hn by l dd de em n ho eo"></div></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://ai.gopubby.com/?source=post_page---byline--61ad3759f06f---------------------------------------" rel="noopener  ugc nofollow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="AI Advances" class="l ep by br ht cx" src="https://miro.medium.com/v2/resize:fill:48:48/1*R8zEd59FDf0l8Re94ImV0Q.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto"/><div class="hn by l br ht em n ho eo"></div></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@karthikrajanv?source=post_page---byline--61ad3759f06f---------------------------------------" rel="noopener follow">Karthik Rajan, Ph.D</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="eq er ab"><div class="bf b bg z dx ab ik"><span class="il l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://ai.gopubby.com/?source=post_page---byline--61ad3759f06f---------------------------------------" rel="noopener  ugc nofollow"><p class="bf b bg z im in io ip iq ir is it bk">AI Advances</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iu iv l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 15, 2024</span></div></span></div></span></div></div></div><div class="ab cp iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl"><div class="h k w ea eb q"><div class="kb l"><div class="ab q kc kd"><div class="pw-multi-vote-icon ed il ke kf kg"><div class=""><div class="kh ki kj kk kl km kn am ko kp kq kg"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"></path><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"></path></svg></div></div></div><div class="pw-multi-vote-count l kr ks kt ku kv kw kx"><p class="bf b dy z dx"><span class="ki">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kh la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="kz"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"></path></svg><p class="bf b dy z dx"><span class="pw-responses-count ky kz">17</span></p></button></div></div></div><div class="ab q jm jn jo jp jq jr js jt ju jv jw jx jy jz ka"><div class="le k j i d"></div><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lf an ao ap id lg lh li" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"></path></svg></button></div></div></div><div class="ep lj cn"><div class="l ae"><div class="ab cb"><div class="lk ll lm ln lo lp ci bh"><div class="ab"><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lf an ao ap id lq lr ld ls lt lu lv lw s lx ly lz ma mb mc md u me mf mg"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"></path></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lf an ao ap id lq lr ld ls lt lu lv lw s lx ly lz ma mb mc md u me mf mg"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"></path></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lf an ao ap id lq lr ld ls lt lu lv lw s lx ly lz ma mb mc md u me mf mg"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"></path></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mh"><div class="ab cb"><div class="lk mi ll mj lm mk cf ml cg mm ci bh"><figure class="mq mr ms mt mu mh mv mw paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mn mo mp"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*2ePkbvke5ObCuJu-g2pzsg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*2ePkbvke5ObCuJu-g2pzsg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*2ePkbvke5ObCuJu-g2pzsg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*2ePkbvke5ObCuJu-g2pzsg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*2ePkbvke5ObCuJu-g2pzsg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*2ePkbvke5ObCuJu-g2pzsg.png 1100w, https://miro.medium.com/v2/resize:fit:2000/format:webp/1*2ePkbvke5ObCuJu-g2pzsg.png 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*2ePkbvke5ObCuJu-g2pzsg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*2ePkbvke5ObCuJu-g2pzsg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*2ePkbvke5ObCuJu-g2pzsg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*2ePkbvke5ObCuJu-g2pzsg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*2ePkbvke5ObCuJu-g2pzsg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*2ePkbvke5ObCuJu-g2pzsg.png 1100w, https://miro.medium.com/v2/resize:fit:2000/1*2ePkbvke5ObCuJu-g2pzsg.png 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px"/><img alt="" class="bh lp nb c" width="1000" height="761" loading="eager" role="presentation"/></picture></div></div><figcaption class="nc nd ne mn mo nf ng bf b bg z dx">Graphical abstract of the integration and key components</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="de28" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk od"><span class="l oe of og bo oh oi oj ok ol ed">R</span>etrieval-augmented generation (RAG) is a powerful tool that equips large language models (LLMs) with the ability to access real-world data for more informed responses. This is achieved by integrating the models with a vector database for real-time learning and adaptation. This feature makes RAG a preferred choice for applications such as chatbots and virtual assistants, where the demand for accurate and sensible responses in real time is high. An advanced variant of this, known as Graph Retrieval-Augmented Generation (GraphRAG), merges the benefits of graph-based knowledge retrieval with LLMs, further enhancing the capabilities in natural language processing. Unlike traditional RAG methods that rely on vector similarity searches, GraphRAG constructs a structured knowledge graph from raw text, capturing entities, relationships, and critical claims. This can enhance LLMs’ ability to understand and synthesize complex datasets and their relationships, yielding more accurate and contextually grounded responses.</p><figure class="mq mr ms mt mu mh mn mo paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mn mo om"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*zhdPp80vsgQcl6c0Pn_AMA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*zhdPp80vsgQcl6c0Pn_AMA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*zhdPp80vsgQcl6c0Pn_AMA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*zhdPp80vsgQcl6c0Pn_AMA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*zhdPp80vsgQcl6c0Pn_AMA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*zhdPp80vsgQcl6c0Pn_AMA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zhdPp80vsgQcl6c0Pn_AMA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*zhdPp80vsgQcl6c0Pn_AMA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*zhdPp80vsgQcl6c0Pn_AMA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*zhdPp80vsgQcl6c0Pn_AMA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*zhdPp80vsgQcl6c0Pn_AMA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*zhdPp80vsgQcl6c0Pn_AMA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*zhdPp80vsgQcl6c0Pn_AMA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*zhdPp80vsgQcl6c0Pn_AMA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" class="bh lp nb c" width="700" height="350" loading="eager" role="presentation"/></picture></div></div><figcaption class="nc nd ne mn mo nf ng bf b bg z dx">Extracted from a paper by Markus Beuhler from MIT (link <a class="af on" href="https://arxiv.org/pdf/2403.11996" rel="noopener ugc nofollow" target="_blank">here</a>)</figcaption></figure><p id="f5c6" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk od"><span class="l oe of og bo oh oi oj ok ol ed">A</span>utoGen is a tool by Microsoft that streamlines the development of intricate applications based on multi-agent LLMs by automating and optimizing workflows that were once complicated and required significant manual effort. Picture AutoGen as a platform where you can interact with multiple GPTs instead of just one. Each GPT acts as an individual “agent”, playing a unique part in a comprehensive operation. Combining GraphRAG’s retrieval strengths with AutoGen AI agents’ conversational and task-oriented functionalities results in robust AI assistants capable of efficiently handling detailed queries, generating and executing codes, creating multi-page scientific reports, and conducting data analysis. Furthermore, offline local LLMs, such as those from Ollama or LM Studio, ensure cost-effective and secure data processing. Local LLMs eliminate the high costs and privacy risks associated with online LLMs, keeping sensitive data within the organization and reducing operational expenses.</p><blockquote class="oo op oq"><p id="fe35" class="nh ni or nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">This article will guide you on constructing a multi-agent AI application with GraphRAG retrieval system, which operates entirely on your local machine and is available at no charge. Here are the key components of this application:</p></blockquote><ol class=""><li id="c46e" class="nh ni fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc os ot ou bk">GraphRAG’s knowledge search methods are integrated with an AutoGen agent via function calling.</li><li id="8961" class="nh ni fq nj b go ov nl nm gr ow no np nq ox ns nt nu oy nw nx ny oz oa ob oc os ot ou bk">GraphRAG (local &amp; global search) is configured to support local models from Ollama for inference and embedding.</li><li id="904f" class="nh ni fq nj b go ov nl nm gr ow no np nq ox ns nt nu oy nw nx ny oz oa ob oc os ot ou bk">AutoGen was extended to support function calling with non-OpenAI LLMs from Ollama via the Lite-LLM proxy server.</li><li id="018f" class="nh ni fq nj b go ov nl nm gr ow no np nq ox ns nt nu oy nw nx ny oz oa ob oc os ot ou bk">Chainlit UI to handle continuous conversations, multi-threading, and user input settings.</li></ol><p id="acbc" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk od"><span class="l oe of og bo oh oi oj ok ol ed">G</span>iven my material science and computational modeling background, I wanted to test this application by constructing knowledge graphs from documentation of ABAQUS, an FEA engineering software, and some technical data sheets of carbon fibers and polymers. The overall accuracy of using the local LLMs could be better, considering the complexity of this dataset. Future articles will explore learnings from benchmark studies using different models for embedding and inference. Nevertheless, I am eager to build more complex knowledge graphs from scientific journals and data in this field, test advanced engineering code generation tasks, and utilize a conversational assistant to brainstorm scientific topics within my expertise. The application looks like this.</p><figure class="mq mr ms mt mu mh mn mo paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mn mo pa"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Emn2loXpcEHw67-mYtASzg.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Emn2loXpcEHw67-mYtASzg.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Emn2loXpcEHw67-mYtASzg.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Emn2loXpcEHw67-mYtASzg.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Emn2loXpcEHw67-mYtASzg.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Emn2loXpcEHw67-mYtASzg.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Emn2loXpcEHw67-mYtASzg.jpeg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*Emn2loXpcEHw67-mYtASzg.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*Emn2loXpcEHw67-mYtASzg.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*Emn2loXpcEHw67-mYtASzg.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*Emn2loXpcEHw67-mYtASzg.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*Emn2loXpcEHw67-mYtASzg.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*Emn2loXpcEHw67-mYtASzg.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*Emn2loXpcEHw67-mYtASzg.jpeg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" class="bh lp nb c" width="700" height="893" loading="lazy" role="presentation"/></picture></div></div><figcaption class="nc nd ne mn mo nf ng bf b bg z dx">Main application UI with example queries. The last two have the same query, but the first is a global search, while the second is a local one.</figcaption></figure><figure class="mq mr ms mt mu mh mn mo paragraph-image"><div class="mn mo pb"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*0Wp0YSLDQsp05W_lycl8vQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*0Wp0YSLDQsp05W_lycl8vQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*0Wp0YSLDQsp05W_lycl8vQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*0Wp0YSLDQsp05W_lycl8vQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*0Wp0YSLDQsp05W_lycl8vQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*0Wp0YSLDQsp05W_lycl8vQ.png 1100w, https://miro.medium.com/v2/resize:fit:846/format:webp/1*0Wp0YSLDQsp05W_lycl8vQ.png 846w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 423px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*0Wp0YSLDQsp05W_lycl8vQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*0Wp0YSLDQsp05W_lycl8vQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*0Wp0YSLDQsp05W_lycl8vQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*0Wp0YSLDQsp05W_lycl8vQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*0Wp0YSLDQsp05W_lycl8vQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*0Wp0YSLDQsp05W_lycl8vQ.png 1100w, https://miro.medium.com/v2/resize:fit:846/1*0Wp0YSLDQsp05W_lycl8vQ.png 846w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 423px"/><img alt="" class="bh lp nb c" width="423" height="444" loading="lazy" role="presentation"/></picture></div><figcaption class="nc nd ne mn mo nf ng bf b bg z dx">Widget settings to switch between local and global search, set community levels, and generation length.</figcaption></figure><p id="e685" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">The development was done in a Linux environment using the Windows Subsystem for Linux (WSL) and Visual Studio Code on a Windows 11 PC with an i9 13th Gen processor, 64 GB RAM, and 24 GB Nvidia RTX 4090. For the best experience in developing and testing this app, it is recommended to use a Linux distribution or WSL. I have not tested this on a native Windows environment. For guidelines on installing WSL and setting up Python and Conda environments, please refer to this article (<a class="af on" href="https://robkerr.ai/fine-tuning-llms-using-a-local-gpu-on-windows/" rel="noopener ugc nofollow" target="_blank">here</a>). Additional references and relevant information are provided at the end of this article.</p><p id="0b3a" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Here is the <a class="af on" href="https://github.com/karthik-codex/autogen_graphRAG" rel="noopener ugc nofollow" target="_blank">link</a> to the source code repository. Now, let’s get started!!</p></div></div></div><div class="ab cb pc pd pe pf" role="separator"><span class="pg by bm ph pi pj"></span><span class="pg by bm ph pi pj"></span><span class="pg by bm ph pi"></span></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="877f" class="pk pl fq bf pm pn po gq pp pq pr gt ps pt pu pv pw px py pz qa qb qc qd qe qf bk">Install model dependencies &amp; clone repository.</h1><h2 id="0299" class="qg pl fq bf pm qh qi qj pp qk ql qm ps nq qn qo qp nu qq qr qs ny qt qu qv qw bk">Install language models from Ollama for inference and embedding</h2><pre class="mq mr ms mt mu qx qy qz bp ra bb bk"><span id="5c1c" class="rb pl fq qy b bg rc rd l re rf"># Mistral for GraphRAG Inference<br/>ollama pull mistral<br/><br/># Nomic-Embed-Text for GraphRAG Embedding<br/>ollama pull nomic-embed-text<br/><br/># LLama3 for Autogen Inference<br/>ollama pull llama3<br/><br/># Host Ollama on a local server: http://localhost:11434<br/>ollama serve</span></pre><h2 id="f007" class="qg pl fq bf pm qh qi qj pp qk ql qm ps nq qn qo qp nu qq qr qs ny qt qu qv qw bk"><strong class="al">Create a conda environment and install these dependencies</strong></h2><pre class="mq mr ms mt mu qx qy qz bp ra bb bk"><span id="83b5" class="rb pl fq qy b bg rc rd l re rf"># Create and activate a conda environment<br/>conda create -n RAG_agents python=3.12<br/>conda activate RAG_agents<br/><br/># Lite-LLM proxy server for Ollama<br/>pip install &#x27;litellm[proxy]&#x27;<br/><br/># Install Ollama<br/>pip install ollama<br/><br/># Microsoft AutoGen<br/>pip install pyautogen &quot;pyautogen[retrievechat]&quot; <br/><br/># Microsoft GraphRAG<br/>pip install graphrag<br/><br/># Text-Token Encoder-Decoder<br/>pip install tiktoken<br/><br/># Chainlit Python application<br/>pip install chainlit<br/><br/># Clone my Git-hub repository<br/>git clone https://github.com/karthik-codex/autogen_graphRAG.git<br/><br/># (BONUS) To Convert PDF files to Markdown for GraphRAG <br/>pip install marker-pdf<br/><br/># (BONUS) Only if you installed Marker-pdf since it removes GPU CUDA support by default<br/>conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia</span></pre><p id="9bf3" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">You will find the following files in my GitHub repository.</p><ol class=""><li id="d0cb" class="nh ni fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc os ot ou bk"><code class="cx rg rh ri qy b">/requirements.txt</code>— <em class="or">Contains a list of all the above packages</em></li><li id="ced6" class="nh ni fq nj b go ov nl nm gr ow no np nq ox ns nt nu oy nw nx ny oz oa ob oc os ot ou bk"><code class="cx rg rh ri qy b">/utils/settings.yaml</code>— <em class="or">Contains the LLM config for using Mistral 7B and Nomic-Text-Embedding from Ollama for GraphRAG offline embedding and indexing.</em> <em class="or">You will use this file to replace the one created when you initialize GraphRAG in your working directory for the first time.</em></li><li id="c051" class="nh ni fq nj b go ov nl nm gr ow no np nq ox ns nt nu oy nw nx ny oz oa ob oc os ot ou bk"><code class="cx rg rh ri qy b">/utils/chainlit_agents.py</code>— <em class="or">Contains class definitions that include AutoGen’s assistant and user proxy agents. This allows multiple agents to be tracked and their messages displayed in the UI. (Shout out to the Chainlit team for building the </em><a class="af on" href="https://github.com/Chainlit/cookbook/blob/main/pyautogen" rel="noopener ugc nofollow" target="_blank"><em class="or">template</em></a><em class="or">).</em></li><li id="721d" class="nh ni fq nj b go ov nl nm gr ow no np nq ox ns nt nu oy nw nx ny oz oa ob oc os ot ou bk"><code class="cx rg rh ri qy b">/utils/embedding.py</code>—<em class="or">Contains the modified embedding functions for GraphRAG embedding for local search queries using Ollama. You will use this file to replace the one inside GraphRAG package (more info below)</em></li><li id="952d" class="nh ni fq nj b go ov nl nm gr ow no np nq ox ns nt nu oy nw nx ny oz oa ob oc os ot ou bk"><code class="cx rg rh ri qy b">utils/openai_embeddings_llm.py</code>—C<em class="or">ontains the modified embedding functions for GraphRAG indexing and embedding using Ollama. You will use this file to replace the one inside GraphRAG package (more info below).</em></li><li id="2a1e" class="nh ni fq nj b go ov nl nm gr ow no np nq ox ns nt nu oy nw nx ny oz oa ob oc os ot ou bk"><code class="cx rg rh ri qy b">/appUI.py</code>—<em class="or">Contains the main asynchronous functions to set up agents, define GraphRAG search functions, track and handle messages, and display them inside Chainlit UI.</em></li><li id="995f" class="nh ni fq nj b go ov nl nm gr ow no np nq ox ns nt nu oy nw nx ny oz oa ob oc os ot ou bk"><code class="cx rg rh ri qy b">/utils/pdf_to_markdown.py</code> — <em class="or">Bonus file containing functions to convert PDF files to markdown files for GraphRAG ingestion.</em></li></ol><h1 id="fc23" class="pk pl fq bf pm pn rj gq pp pq rk gt ps pt rl pv pw px rm pz qa qb rn qd qe qf bk">Create a GraphRAG knowledge base.</h1><h2 id="57e7" class="qg pl fq bf pm qh qi qj pp qk ql qm ps nq qn qo qp nu qq qr qs ny qt qu qv qw bk"><strong class="al">Initialize GraphRAG in the root folder of the repository</strong></h2><pre class="mq mr ms mt mu qx qy qz bp ra bb bk"><span id="5641" class="rb pl fq qy b bg rc rd l re rf">#make a new folder &quot;input&quot; to place your input files for GraphRAG (.txt or .md)<br/>mkdir -p ./input<br/><br/># Initialize GraphRAG to create the required files and folders in the root dir<br/>python -m graphrag.index --init  --root .<br/><br/># Move the settings.yaml file to replace the one created by GraphRAG --init<br/>mv ./utils/settings.yaml ./</span></pre><h2 id="acf5" class="qg pl fq bf pm qh qi qj pp qk ql qm ps nq qn qo qp nu qq qr qs ny qt qu qv qw bk"><strong class="al">Configure GraphRAG settings to support local models from Ollama</strong></h2><p id="9652" class="pw-post-body-paragraph nh ni fq nj b go ro nl nm gr rp no np nq rq ns nt nu rr nw nx ny rs oa ob oc fj bk">Below is a snippet from <code class="cx rg rh ri qy b">settings.yaml</code> illustrating the configuration of LLMs for creating indexes and embeddings. GraphRAG requires a 32k context length for indexing, making Mistral the chosen model. For embeddings, Nomic-embed-text is selected, although you can experiment with other embeddings from Ollama. No need to set <code class="cx rg rh ri qy b">${GRAPHRAG_API_KEY}</code>, as access is not required to these local models’ endpoints.</p><pre class="mq mr ms mt mu qx qy qz bp ra bb bk"><span id="3d54" class="rb pl fq qy b bg rc rd l re rf">encoding_model: cl100k_base<br/>skip_workflows: []<br/>llm:<br/>  api_key: ${GRAPHRAG_API_KEY}<br/>  type: openai_chat # or azure_openai_chat<br/>  model: mistral<br/>  model_supports_json: true<br/>  api_base: http://localhost:11434/v1 <br/>.<br/>.<br/>.<br/>embeddings:<br/>  async_mode: threaded # or asyncio<br/>  llm:<br/>    api_key: ${GRAPHRAG_API_KEY}<br/>    type: openai_embedding # or azure_openai_embedding<br/>    model: nomic_embed_text<br/>    api_base: http://localhost:11434/api <br/>.<br/>.<br/>.<br/>input:  #Change input file pattern to.md, or .txt<br/>  type: file # or blob<br/>  file_type: text # or csv<br/>  base_dir: &quot;input&quot;<br/>  file_encoding: utf-8<br/>  file_pattern: &quot;.*\\.md$&quot;</span></pre><p id="8283" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">You can specify the folder containing the input files in the “input” folder in the root directory. Both text and markdown files can be used. You can use the <code class="cx rg rh ri qy b">/utils/pdf_to_markdown.py</code> to convert your PDFs to markdown files that are then placed inside the “input” folder. Handling multiple file formats has not been figured out, but it is a solvable issue.</p><p id="4437" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Before running GraphRAG to index, create embeddings, and perform local queries, you must modify the Python files <code class="cx rg rh ri qy b">openai_embeddings_llm.py </code>and <code class="cx rg rh ri qy b">embedding.py</code>located within the GraphRAG package. Without this modification, GraphRAG will throw an error when creating embeddings, as it won&#x27;t recognize &quot;nomic-embed-text&quot; as a valid embedding model from Ollama. In my setup, these files are located at <code class="cx rg rh ri qy b">/home/karthik/miniconda3/envs/RAG_agents/lib/python3.12/site-packages/graphrag/llm/openai/openai_embeddings_llm.py</code>and <code class="cx rg rh ri qy b">/home/karthik/miniconda3/envs/RAG_agents/lib/python3.12/site-packages/graphrag/query/llm/oai/embedding.py</code></p><p id="08e9" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">You can locate these files using the command <code class="cx rg rh ri qy b">sudo find / -name openai_embeddings_llm.py</code>.</p><h2 id="bae6" class="qg pl fq bf pm qh qi qj pp qk ql qm ps nq qn qo qp nu qq qr qs ny qt qu qv qw bk">Create embeddings and knowledge graphs.</h2><p id="f7a5" class="pw-post-body-paragraph nh ni fq nj b go ro nl nm gr rp no np nq rq ns nt nu rr nw nx ny rs oa ob oc fj bk">Lastly, we create the embeddings and test the knowledge graph using the global or local search method. After completing the embedding process, you can find the output artifacts (.parquet files) and reports (.json and .logs) in the “output” folder of your GraphRAG working directory, which is the root folder in this instance.</p><pre class="mq mr ms mt mu qx qy qz bp ra bb bk"><span id="fd03" class="rb pl fq qy b bg rc rd l re rf"># Create knowledge graph - this takes some time<br/>python -m graphrag.index --root .<br/><br/># Test GraphRAG<br/>python -m graphrag.query --root . --method global &quot;&lt;insert your query&gt;&quot;</span></pre><h2 id="664c" class="qg pl fq bf pm qh qi qj pp qk ql qm ps nq qn qo qp nu qq qr qs ny qt qu qv qw bk">Start the Lite-LLM server and run the app from the terminal</h2><p id="7ce7" class="pw-post-body-paragraph nh ni fq nj b go ro nl nm gr rp no np nq rq ns nt nu rr nw nx ny rs oa ob oc fj bk">Below is the command to initialize the server before running the app. I chose Llama3:8b to test this app. You can use larger models if your hardware permits. More information on Lite-LLM can be found at this <a class="af on" href="https://microsoft.github.io/autogen-for-net/articles/Function-call-with-ollama-and-litellm.html" rel="noopener ugc nofollow" target="_blank">link</a>. Now, you are ready to run the application from another terminal. Make sure you are in the right conda environment.</p><pre class="mq mr ms mt mu qx qy qz bp ra bb bk"><span id="78e4" class="rb pl fq qy b bg rc rd l re rf"># start server from terminal<br/>litellm --model ollama_chat/llama3<br/><br/># run app from another terminal<br/>chainlit run appUI.py</span></pre><h1 id="6a6d" class="pk pl fq bf pm pn rj gq pp pq rk gt ps pt rl pv pw px rm pz qa qb rn qd qe qf bk">Breakdown: Core components of appUI.py</h1><h2 id="97ab" class="qg pl fq bf pm qh qi qj pp qk ql qm ps nq qn qo qp nu qq qr qs ny qt qu qv qw bk">Import python libraries</h2><pre class="mq mr ms mt mu qx qy qz bp ra bb bk"><span id="1aea" class="rb pl fq qy b bg rc rd l re rf">import autogen<br/>from rich import print<br/>import chainlit as cl<br/>from typing_extensions import Annotated<br/>from chainlit.input_widget import (<br/>   Select, Slider, Switch)<br/>from autogen import AssistantAgent, UserProxyAgent<br/>from utils.chainlit_agents import ChainlitUserProxyAgent, ChainlitAssistantAgent<br/>from graphrag.query.cli import run_global_search, run_local_search</span></pre><p id="36d1" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">You will notice two classes being imported from <em class="or">chainlit_agents</em>. These wrapper classes for AutoGen agents enable Chainlit to track their conversations and handle termination or other user inputs. You can read more about this <a class="af on" href="https://medium.com/@antoineross/autogen-web-application-using-chainlit-8c5ebf5a4e75" rel="noopener">here</a>.</p><h2 id="359d" class="qg pl fq bf pm qh qi qj pp qk ql qm ps nq qn qo qp nu qq qr qs ny qt qu qv qw bk">Configure AutoGen agents</h2><p id="760d" class="pw-post-body-paragraph nh ni fq nj b go ro nl nm gr rp no np nq rq ns nt nu rr nw nx ny rs oa ob oc fj bk">The AutoGen agents utilize models from Ollama via the Lite-LLM proxy server. This is necessary because AutoGen does not support function calling through non-OpenAI inference models. The proxy server enables using Ollama models for function calling and code execution.</p><pre class="mq mr ms mt mu qx qy qz bp ra bb bk"><span id="4f90" class="rb pl fq qy b bg rc rd l re rf"># LLama3 LLM from Lite-LLM Server for Agents #<br/>llm_config_autogen = {<br/>    &quot;seed&quot;: 40,  # change the seed for different trials<br/>    &quot;temperature&quot;: 0,<br/>    &quot;config_list&quot;: [{&quot;model&quot;: &quot;litellm&quot;, <br/>                     &quot;base_url&quot;: &quot;http://0.0.0.0:4000/&quot;, <br/>                     &#x27;api_key&#x27;: &#x27;ollama&#x27;},<br/>    ],<br/>    &quot;timeout&quot;: 60000,<br/>}</span></pre><h2 id="bb59" class="qg pl fq bf pm qh qi qj pp qk ql qm ps nq qn qo qp nu qq qr qs ny qt qu qv qw bk">Instantiate agents and input user settings at the start of the chat</h2><p id="3a3f" class="pw-post-body-paragraph nh ni fq nj b go ro nl nm gr rp no np nq rq ns nt nu rr nw nx ny rs oa ob oc fj bk">I created three Chainlit widgets (switch, select, and slider) as user settings to choose the GraphRAG search type, community level, and content generation type. When turned ON, the switch widget uses the GraphRAG local search method for querying. The select options for content generation include “prioritized list,” “single paragraph,” “multiple paragraphs,” and “multiple-page report.” The slider widget selects the community generation level with options 0, 1, and 2. You can read more about the GraphRAG communities <a class="af on" href="https://mlnotes.substack.com/p/graphrag-combining-knowledge-graphs" rel="noopener ugc nofollow" target="_blank">here</a>.</p><pre class="mq mr ms mt mu qx qy qz bp ra bb bk"><span id="9be9" class="rb pl fq qy b bg rc rd l re rf">@cl.on_chat_start<br/>async def on_chat_start():<br/>  try:<br/>    settings = await cl.ChatSettings(<br/>            [      <br/>                Switch(id=&quot;Search_type&quot;, label=&quot;(GraphRAG) Local Search&quot;, initial=True),       <br/>                Select(<br/>                    id=&quot;Gen_type&quot;,<br/>                    label=&quot;(GraphRAG) Content Type&quot;,<br/>                    values=[&quot;prioritized list&quot;, &quot;single paragraph&quot;, &quot;multiple paragraphs&quot;, &quot;multiple-page report&quot;],<br/>                    initial_index=1,<br/>                ),          <br/>                Slider(<br/>                    id=&quot;Community&quot;,<br/>                    label=&quot;(GraphRAG) Community Level&quot;,<br/>                    initial=0,<br/>                    min=0,<br/>                    max=2,<br/>                    step=1,<br/>                ),<br/><br/>            ]<br/>        ).send()<br/><br/>    response_type = settings[&quot;Gen_type&quot;]<br/>    community = settings[&quot;Community&quot;]<br/>    local_search = settings[&quot;Search_type&quot;]<br/>    <br/>    cl.user_session.set(&quot;Gen_type&quot;, response_type)<br/>    cl.user_session.set(&quot;Community&quot;, community)<br/>    cl.user_session.set(&quot;Search_type&quot;, local_search)<br/><br/>    retriever   = AssistantAgent(<br/>       name=&quot;Retriever&quot;, <br/>       llm_config=llm_config_autogen, <br/>       system_message=&quot;&quot;&quot;Only execute the function query_graphRAG to look for context. <br/>                    Output &#x27;TERMINATE&#x27; when an answer has been provided.&quot;&quot;&quot;,<br/>       max_consecutive_auto_reply=1,<br/>       human_input_mode=&quot;NEVER&quot;, <br/>       description=&quot;Retriever Agent&quot;<br/>    )<br/><br/>    user_proxy = ChainlitUserProxyAgent(<br/>        name=&quot;User_Proxy&quot;,<br/>        human_input_mode=&quot;ALWAYS&quot;,<br/>        llm_config=llm_config_autogen,<br/>        is_termination_msg=lambda x: x.get(&quot;content&quot;, &quot;&quot;).rstrip().endswith(&quot;TERMINATE&quot;),<br/>        code_execution_config=False,<br/>        system_message=&#x27;&#x27;&#x27;A human admin. Interact with the retriever to provide any context&#x27;&#x27;&#x27;,<br/>        description=&quot;User Proxy Agent&quot;<br/>    )<br/>    <br/>    print(&quot;Set agents.&quot;)<br/><br/>    cl.user_session.set(&quot;Query Agent&quot;, user_proxy)<br/>    cl.user_session.set(&quot;Retriever&quot;, retriever)<br/><br/>    msg = cl.Message(content=f&quot;&quot;&quot;Hello! What task would you like to get done today?      <br/>                     &quot;&quot;&quot;, <br/>                     author=&quot;User_Proxy&quot;)<br/>    await msg.send()<br/><br/>    print(&quot;Message sent.&quot;)<br/>    <br/>  except Exception as e:<br/>    print(&quot;Error: &quot;, e)<br/>    pass</span></pre><p id="ecec" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">I chose not to use the Chainlit wrapper class for the retriever assistant agent. This allowed me to disable tracking of the retriever’s output and directly capture the response from the GraphRAG function. The reason is that when the response passes through the retriever, the text loses its formatting, including spaces and paragraph indents. This issue was especially noticeable when generating multi-page reports with main and sub-headings. I could preserve the original formatting by bypassing the Chainlit wrapper and directly retrieving the output from the GraphRAG function. You will see how I achieved this below.</p><h2 id="7e48" class="qg pl fq bf pm qh qi qj pp qk ql qm ps nq qn qo qp nu qq qr qs ny qt qu qv qw bk">Update changes in input settings</h2><p id="edae" class="pw-post-body-paragraph nh ni fq nj b go ro nl nm gr rp no np nq rq ns nt nu rr nw nx ny rs oa ob oc fj bk">This function detects any changes made to the select, switch, and slider widgets from settings so it can reflect those changes in the subsequent queries.</p><pre class="mq mr ms mt mu qx qy qz bp ra bb bk"><span id="921a" class="rb pl fq qy b bg rc rd l re rf">@cl.on_settings_update<br/>async def setup_agent(settings):<br/>    response_type = settings[&quot;Gen_type&quot;]<br/>    community = settings[&quot;Community&quot;]<br/>    local_search = settings[&quot;Search_type&quot;]<br/>    cl.user_session.set(&quot;Gen_type&quot;, response_type)<br/>    cl.user_session.set(&quot;Community&quot;, community)<br/>    cl.user_session.set(&quot;Search_type&quot;, local_search)<br/>    print(&quot;on_settings_update&quot;, settings)</span></pre><h2 id="5f6d" class="qg pl fq bf pm qh qi qj pp qk ql qm ps nq qn qo qp nu qq qr qs ny qt qu qv qw bk">Update UI with incoming messages from agents and the user.</h2><p id="4ea6" class="pw-post-body-paragraph nh ni fq nj b go ro nl nm gr rp no np nq rq ns nt nu rr nw nx ny rs oa ob oc fj bk">This is the core part of the application, which creates a group chat with two agents, defines a function “state_transition” to manage the conversation sequence, and provides the asynchronous RAG query function.</p><p id="0a1a" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">You will notice <code class="cx rg rh ri qy b">INPUT_DIR ,ROOT_DIR,</code> <code class="cx rg rh ri qy b">RESPONSE_TYPE,</code> <code class="cx rg rh ri qy b">COMMUNTIY</code> parameters that are passed into the local and global search GraphRAG query functions based on the bool parameter<code class="cx rg rh ri qy b">LOCAL_SEARCH</code>. The<code class="cx rg rh ri qy b">ROOT_DIR,</code> is set to <code class="cx rg rh ri qy b">’.’</code> — pay attention to this if you initialized GraphRAG in a different directory.</p><p id="edb0" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">The asynchronous function “query_graphRAG” calls the GraphRAG global or local search method. You will notice the line <code class="cx rg rh ri qy b">await cl.Message(content=result.response).send()</code> inside the <code class="cx rg rh ri qy b">async def query_graphRAG</code> function that directly retrieves the output from the RAG query and preserves the text formatting of the retrieved content.</p><pre class="mq mr ms mt mu qx qy qz bp ra bb bk"><span id="61ea" class="rb pl fq qy b bg rc rd l re rf">@cl.on_message<br/>async def run_conversation(message: cl.Message):<br/>    print(&quot;Running conversation&quot;)<br/>    CONTEXT = message.content<br/><br/>    MAX_ITER = 10<br/>    INPUT_DIR = None<br/>    ROOT_DIR = &#x27;.&#x27;<br/>    RESPONSE_TYPE = cl.user_session.get(&quot;Gen_type&quot;)<br/>    COMMUNITY = cl.user_session.get(&quot;Community&quot;)<br/>    LOCAL_SEARCH = cl.user_session.get(&quot;Search_type&quot;)<br/><br/>    print(&quot;Setting groupchat&quot;)<br/><br/>    retriever   = cl.user_session.get(&quot;Retriever&quot;)<br/>    user_proxy  = cl.user_session.get(&quot;Query Agent&quot;)<br/><br/>    def state_transition(last_speaker, groupchat):<br/>        messages = groupchat.messages<br/>        if last_speaker is user_proxy:<br/>            return retriever<br/>        if last_speaker is retriever:<br/>            if messages[-1][&quot;content&quot;].lower() not in [&#x27;math_expert&#x27;,&#x27;physics_expert&#x27;]:<br/>                return user_proxy<br/>            else:<br/>                if messages[-1][&quot;content&quot;].lower() == &#x27;math_expert&#x27;:<br/>                    return user_proxy<br/>                else:<br/>                    return user_proxy<br/>        else:<br/>            pass<br/>            return None<br/><br/>    async def query_graphRAG(<br/>          question: Annotated[str, &#x27;Query string containing information that you want from RAG search&#x27;]<br/>                          ) -&gt; str:<br/>        if LOCAL_SEARCH:<br/>            result = run_local_search(INPUT_DIR, ROOT_DIR, COMMUNITY ,RESPONSE_TYPE, question)<br/>        else:<br/>            result = run_global_search(INPUT_DIR, ROOT_DIR, COMMUNITY ,RESPONSE_TYPE, question)<br/>        await cl.Message(content=result).send()<br/>        return result<br/><br/>    for caller in [retriever]:<br/>        d_retrieve_content = caller.register_for_llm(<br/>            description=&quot;retrieve content for code generation and question answering.&quot;, api_style=&quot;function&quot;<br/>        )(query_graphRAG)<br/><br/>    for agents in [user_proxy, retriever]:<br/>        agents.register_for_execution()(d_retrieve_content)<br/><br/>    groupchat = autogen.GroupChat(<br/>        agents=[user_proxy, retriever],<br/>        messages=[],<br/>        max_round=MAX_ITER,<br/>        speaker_selection_method=state_transition,<br/>        allow_repeat_speaker=True,<br/>    )<br/>    manager = autogen.GroupChatManager(groupchat=groupchat,<br/>                                       llm_config=llm_config_autogen, <br/>                                       is_termination_msg=lambda x: x.get(&quot;content&quot;, &quot;&quot;) and x.get(&quot;content&quot;, &quot;&quot;).rstrip().endswith(&quot;TERMINATE&quot;),<br/>                                       code_execution_config=False,<br/>                                       )    <br/><br/># -------------------- Conversation Logic. Edit to change your first message based on the Task you want to get done. ----------------------------- # <br/>    if len(groupchat.messages) == 0: <br/>      await cl.make_async(user_proxy.initiate_chat)( manager, message=CONTEXT, )<br/>    elif len(groupchat.messages) &lt; MAX_ITER:<br/>      await cl.make_async(user_proxy.send)( manager, message=CONTEXT, )<br/>    elif len(groupchat.messages) == MAX_ITER:  <br/>      await cl.make_async(user_proxy.send)( manager, message=&quot;exit&quot;, )</span></pre><p id="ab32" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">For this application, we only need two agents. You can add/modify agents and configure the “state_transition” function to orchestrate speaker selection in conversations for more complex workflows.</p></div></div></div><div class="ab cb pc pd pe pf" role="separator"><span class="pg by bm ph pi pj"></span><span class="pg by bm ph pi pj"></span><span class="pg by bm ph pi"></span></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="f471" class="pk pl fq bf pm pn po gq pp pq pr gt ps pt pu pv pw px py pz qa qb qc qd qe qf bk">Final Thoughts</h1><p id="46ec" class="pw-post-body-paragraph nh ni fq nj b go ro nl nm gr rp no np nq rq ns nt nu rr nw nx ny rs oa ob oc fj bk">This is my first venture into AI agents, LLMs, and RAGs, and I dove straight into creating this implementation over the past few weeks, bypassing many basics. While this implementation is imperfect, it is an excellent template for developing more complex applications. It lays a solid foundation for integrating multiple functions and coding agents and should enable you to build sophisticated workflows, customize agent interactions, and enhance functionality as needed.</p><blockquote class="oo op oq"><p id="87b3" class="nh ni or nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><strong class="nj fr">About Me</strong>: I am a lead modeling engineer at Eaton Research Labs, Southfield, MI, USA. I explore, develop tools, and write about things at the intersection of computational mechanics, material science, engineering, language models, and generative AI.</p></blockquote><p id="de72" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">If you want to stay updated, follow me on my socials below.</p><p id="d58e" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Socials: <a class="af on" href="https://www.linkedin.com/in/karthik-rajan-venkatesan/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>, <a class="af on" href="https://github.com/karthik-codex" rel="noopener ugc nofollow" target="_blank">GitHub</a>, <a class="af on" href="https://github.com/karthik-codex/autogen_graphRAG.git" rel="noopener ugc nofollow" target="_blank">Source Code</a></p><h1 id="db00" class="pk pl fq bf pm pn rj gq pp pq rk gt ps pt rl pv pw px rm pz qa qb rn qd qe qf bk">Some Useful References</h1><ol class=""><li id="a657" class="nh ni fq nj b go ro nl nm gr rp no np nq rq ns nt nu rr nw nx ny rs oa ob oc os ot ou bk"><a class="af on" href="https://medium.com/@datadrifters/autogen-litellm-and-open-source-llms-c4c6bc8fa9c5" rel="noopener">https://medium.com/@datadrifters/autogen-litellm-and-open-source-llms-c4c6bc8fa9c5</a></li><li id="4b49" class="nh ni fq nj b go ov nl nm gr ow no np nq ox ns nt nu oy nw nx ny oz oa ob oc os ot ou bk"><a class="af on" href="https://medium.com/@rajib76.gcp/memory-default-compute-fallback-5ff4287d47e6" rel="noopener">https://medium.com/@rajib76.gcp/memory-default-compute-fallback-5ff4287d47e6</a></li><li id="29b7" class="nh ni fq nj b go ov nl nm gr ow no np nq ox ns nt nu oy nw nx ny oz oa ob oc os ot ou bk"><a class="af on" href="https://medium.com/generative-ai/graphrag-the-rag-approach-by-microsoft-e1abc7eb9fba" rel="noopener">https://medium.com/generative-ai/graphrag-the-rag-approach-by-microsoft-e1abc7eb9fba</a></li><li id="a49c" class="nh ni fq nj b go ov nl nm gr ow no np nq ox ns nt nu oy nw nx ny oz oa ob oc os ot ou bk"><a class="af on" href="https://docs.chainlit.io/get-started/overview" rel="noopener ugc nofollow" target="_blank">https://docs.chainlit.io/get-started/overview</a></li><li id="ff7a" class="nh ni fq nj b go ov nl nm gr ow no np nq ox ns nt nu oy nw nx ny oz oa ob oc os ot ou bk"><a class="af on" href="https://medium.com/@antoineross/autogen-web-application-using-chainlit-8c5ebf5a4e75" rel="noopener">https://medium.com/@antoineross/autogen-web-application-using-chainlit-8c5ebf5a4e75</a></li></ol></div></div></div></div></section></div></div></article></div><div class="ab cb"><div class="ci bh ev ew ex ey"><div class="rt ru ab ij"><div class="rv ab"><a class="rw ay am ao" href="https://medium.com/tag/autogen?source=post_page-----61ad3759f06f---------------------------------------" rel="noopener follow"><div class="rx ed cx ry fa rz sa bf b bg z bk sb">Autogen</div></a></div><div class="rv ab"><a class="rw ay am ao" href="https://medium.com/tag/ollama?source=post_page-----61ad3759f06f---------------------------------------" rel="noopener follow"><div class="rx ed cx ry fa rz sa bf b bg z bk sb">Ollama</div></a></div><div class="rv ab"><a class="rw ay am ao" href="https://medium.com/tag/chainlit?source=post_page-----61ad3759f06f---------------------------------------" rel="noopener follow"><div class="rx ed cx ry fa rz sa bf b bg z bk sb">Chainlit</div></a></div><div class="rv ab"><a class="rw ay am ao" href="https://medium.com/tag/rags?source=post_page-----61ad3759f06f---------------------------------------" rel="noopener follow"><div class="rx ed cx ry fa rz sa bf b bg z bk sb">Rags</div></a></div><div class="rv ab"><a class="rw ay am ao" href="https://medium.com/tag/graphrag?source=post_page-----61ad3759f06f---------------------------------------" rel="noopener follow"><div class="rx ed cx ry fa rz sa bf b bg z bk sb">Graphrag</div></a></div></div></div></div><div class="l"></div><footer class="sc sd se sf sg ab q sh hs c"><div class="l ae"><div class="ab cb"><div class="ci bh ev ew ex ey"><div class="ab cp si"><div class="ab q kc"><div class="sj l"><span class="l sk sl sm e d"><div class="ab q kc kd"><div class="pw-multi-vote-icon ed il ke kf kg"><div class=""><div class="kh ki kj kk kl km kn am ko kp kq kg"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"></path><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"></path></svg></div></div></div><div class="pw-multi-vote-count l kr ks kt ku kv kw kx"><p class="bf b dy z dx"><span class="ki">--</span></p></div></div></span><span class="l h g f sn so"><div class="ab q kc kd"><div class="pw-multi-vote-icon ed il ke kf kg"><div class=""><div class="kh ki kj kk kl km kn am ko kp kq kg"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"></path><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"></path></svg></div></div></div><div class="pw-multi-vote-count l kr ks kt ku kv kw kx"><p class="bf b dy z dx"><span class="ki">--</span></p></div></div></span></div><div class="bq ab"><div><div class="bm" aria-hidden="false"><button class="ao kh la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="kz"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"></path></svg><p class="bf b bg z dx"><span class="pw-responses-count ky kz">17</span></p></button></div></div></div></div><div class="ab q"><div class="pj l ig"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="footerBookmarkButton" class="af ee ah ai aj ak al lf an ao ap id lg lh li" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"></path></svg></button></div></div></div><div class="pj l ig"><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="footerSocialShareButton" class="af ee ah ai aj ak al lf an ao ap id lq lr ld ls"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"></path></svg></button></div></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="footerStoryOptionsButton" class="af ee ah ai aj ak al lf an ao ap id lq lr ld ls"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></footer><div class="sp l"><div class="ab cb"><div class="ci bh ev ew ex ey"><div class="sq l"><div class="ab sr ss st ii ih"><div class="su sv sw sx sy sz ta tb tc td ab cp"><div class="h k"><a href="https://ai.gopubby.com/?source=post_page---post_publication_info--61ad3759f06f---------------------------------------" rel="noopener follow"><div class="ed ab"><img alt="AI Advances" class="te hj hk cx" src="https://miro.medium.com/v2/resize:fill:96:96/1*R8zEd59FDf0l8Re94ImV0Q.png" width="48" height="48" loading="lazy"/><div class="te l hk hj em n el tf"></div></div></a></div><div class="j i d"><a href="https://ai.gopubby.com/?source=post_page---post_publication_info--61ad3759f06f---------------------------------------" rel="noopener follow"><div class="ed ab"><img alt="AI Advances" class="te th tg cx" src="https://miro.medium.com/v2/resize:fill:128:128/1*R8zEd59FDf0l8Re94ImV0Q.png" width="64" height="64" loading="lazy"/><div class="te l tg th em n el tf"></div></div></a></div><div class="j i d ti ig"><div class="ab"><button class="bf b bg z tj rx tk tl tm tn to tp tq tr ts tt tu tv tw tx ty ep bm tz nd">Follow</button></div></div></div><div class="ab co ua"><div class="ub uc ud mj mi l"><a class="af ag ah aj ak al am an ao ap aq ar as at ab q" href="https://ai.gopubby.com/?source=post_page---post_publication_info--61ad3759f06f---------------------------------------" rel="noopener follow"><h2 class="pw-author-name bf uf ug uh ui uj uk ul nq qo qp nu qr qs ny qu qv bk"><span class="fj ue">Published in <!-- -->AI Advances</span></h2></a><div class="rv ab hi"><div class="l ig"><span class="pw-follower-count bf b bg z dx"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" rel="noopener follow" href="/followers?source=post_page---post_publication_info--61ad3759f06f---------------------------------------">23K Followers</a></span></div><div class="bf b bg z dx ab ik"><span class="hz l" aria-hidden="true"><span class="bf b bg z dx">·</span></span><a class="af ag ah ai aj ak al am an ao ap aq ar hy" rel="noopener follow" href="/the-ai-race-everyones-getting-wrong-2e3473bc6f46?source=post_page---post_publication_info--61ad3759f06f---------------------------------------">Last published <!-- -->14 hours ago</a></div></div><div class="um l"><p class="bf b bg z bk"><span class="fj">Democratizing access to artificial intelligence</span></p></div></div></div><div class="h k"><div class="ab"><button class="bf b bg z tj rx tk tl tm tn to tp tq tr ts tt tu tv tw tx ty ep bm tz nd">Follow</button></div></div></div></div><div class="ab sr ss st ii ih"><div class="su sv sw sx sy sz ta tb tc td ab cp"><div class="h k"><a tabindex="0" href="https://medium.com/@karthikrajanv?source=post_page---post_author_info--61ad3759f06f---------------------------------------" rel="noopener follow"><div class="l ed"><img alt="Karthik Rajan, Ph.D" class="l ep by hk hj cx" src="https://miro.medium.com/v2/resize:fill:96:96/1*mGw0B5wtYwJDrrVhxaL2NA.jpeg" width="48" height="48" loading="lazy"/><div class="el by l hk hj em n ay tf"></div></div></a></div><div class="j i d"><a tabindex="0" href="https://medium.com/@karthikrajanv?source=post_page---post_author_info--61ad3759f06f---------------------------------------" rel="noopener follow"><div class="l ed"><img alt="Karthik Rajan, Ph.D" class="l ep by tg th cx" src="https://miro.medium.com/v2/resize:fill:128:128/1*mGw0B5wtYwJDrrVhxaL2NA.jpeg" width="64" height="64" loading="lazy"/><div class="el by l tg th em n ay tf"></div></div></a></div><div class="j i d ti ig"><div class="ab"><button class="bf b bg z tj rx tk tl tm tn to tp tq tr ts tt tu tv tw tx ty ep bm tz nd">Follow</button></div></div></div><div class="ab co ua"><div class="ub uc ud mj mi l"><a class="af ag ah aj ak al am an ao ap aq ar as at ab q" href="https://medium.com/@karthikrajanv?source=post_page---post_author_info--61ad3759f06f---------------------------------------" rel="noopener follow"><h2 class="pw-author-name bf uf ug uh ui uj uk ul nq qo qp nu qr qs ny qu qv bk"><span class="fj ue">Written by <!-- -->Karthik Rajan, Ph.D</span></h2></a><div class="rv ab hi"><div class="l ig"><span class="pw-follower-count bf b bg z dx"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" href="https://medium.com/@karthikrajanv/followers?source=post_page---post_author_info--61ad3759f06f---------------------------------------" rel="noopener follow">970 Followers</a></span></div><div class="bf b bg z dx ab ik"><span class="hz l" aria-hidden="true"><span class="bf b bg z dx">·</span></span><a class="af ag ah ai aj ak al am an ao ap aq ar hy" href="https://medium.com/@karthikrajanv/following?source=post_page---post_author_info--61ad3759f06f---------------------------------------" rel="noopener follow">10 Following</a></div></div><div class="um l"><p class="bf b bg z bk"><span class="fj">I explore, develop tools &amp; write about things at the intersection of computational mechanics, material science, engineering, &amp; generative AI.</span></p></div></div></div><div class="h k"><div class="ab"><button class="bf b bg z tj rx tk tl tm tn to tp tq tr ts tt tu tv tw tx ty ep bm tz nd">Follow</button></div></div></div></div></div></div><div class="un uo up uq ur l"><div class="us bh r sp"></div><div class="ab cb"><div class="ci bh ev ew ex ey"><div class="ab q cp"><h2 class="bf uf pn gq pp pq gt ps pt pv pw px pz qa qb qd qe bk">Responses (<!-- -->17<!-- -->)</h2><div class="ab ut"><div><div class="bm" aria-hidden="false"><a class="uu uv" href="https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--61ad3759f06f---------------------------------------" rel="noopener follow" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" viewBox="0 0 25 25"><path fill-rule="evenodd" d="M11.987 5.036a.754.754 0 0 1 .914-.01c.972.721 1.767 1.218 2.6 1.543.828.322 1.719.485 2.887.505a.755.755 0 0 1 .741.757c-.018 3.623-.43 6.256-1.449 8.21-1.034 1.984-2.662 3.209-4.966 4.083a.75.75 0 0 1-.537-.003c-2.243-.874-3.858-2.095-4.897-4.074-1.024-1.951-1.457-4.583-1.476-8.216a.755.755 0 0 1 .741-.757c1.195-.02 2.1-.182 2.923-.503.827-.322 1.6-.815 2.519-1.535m.468.903c-.897.69-1.717 1.21-2.623 1.564-.898.35-1.856.527-3.026.565.037 3.45.469 5.817 1.36 7.515.884 1.684 2.25 2.762 4.284 3.571 2.092-.81 3.465-1.89 4.344-3.575.886-1.698 1.299-4.065 1.334-7.512-1.149-.039-2.091-.217-2.99-.567-.906-.353-1.745-.873-2.683-1.561m-.009 9.155a2.672 2.672 0 1 0 0-5.344 2.672 2.672 0 0 0 0 5.344m0 1a3.672 3.672 0 1 0 0-7.344 3.672 3.672 0 0 0 0 7.344m-1.813-3.777.525-.526.916.917 1.623-1.625.526.526-2.149 2.152z" clip-rule="evenodd"></path></svg></a></div></div></div></div><div class="uw ux uy uz va vb vc l"></div><div class="pc l"><button class="bf b bg z bk rx vd ve vf vg lc vh vi ts id vj vk vl tw vm vn vo vp vq tx ty ep bm tz nd">See all responses</button></div></div></div></div><div class="vr vs vt vu vv l bx"><div class="h k j"><div class="us bh vw vx"></div><div class="ab cb"><div class="ci bh ev ew ex ey"><div class="vy ab kc ij"><div class="vz wa l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://help.medium.com/hc/en-us?source=post_page-----61ad3759f06f---------------------------------------" rel="noopener follow"><p class="bf b dy z dx">Help</p></a></div><div class="vz wa l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.statuspage.io/?source=post_page-----61ad3759f06f---------------------------------------" rel="noopener follow"><p class="bf b dy z dx">Status</p></a></div><div class="vz wa l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/about?autoplay=1&amp;source=post_page-----61ad3759f06f---------------------------------------" rel="noopener follow"><p class="bf b dy z dx">About</p></a></div><div class="vz wa l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----61ad3759f06f---------------------------------------" rel="noopener follow"><p class="bf b dy z dx">Careers</p></a></div><div class="vz wa l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="mailto:pressinquiries@medium.com" rel="noopener follow"><p class="bf b dy z dx">Press</p></a></div><div class="vz wa l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://blog.medium.com/?source=post_page-----61ad3759f06f---------------------------------------" rel="noopener follow"><p class="bf b dy z dx">Blog</p></a></div><div class="vz wa l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----61ad3759f06f---------------------------------------" rel="noopener follow"><p class="bf b dy z dx">Privacy</p></a></div><div class="vz wa l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----61ad3759f06f---------------------------------------" rel="noopener follow"><p class="bf b dy z dx">Terms</p></a></div><div class="vz wa l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://speechify.com/medium?source=post_page-----61ad3759f06f---------------------------------------" rel="noopener follow"><p class="bf b dy z dx">Text to speech</p></a></div><div class="vz l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/business?source=post_page-----61ad3759f06f---------------------------------------" rel="noopener follow"><p class="bf b dy z dx">Teams</p></a></div></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__="main-20250224-082317-7f65a8da54"</script><script>window.__GRAPHQL_URI__ = "https://ai.gopubby.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"algolia":{"queries":{}},"cache":{"experimentGroupSet":true,"reason":"User is logged in","group":"disabled","tags":["group-edgeCachePosts","post-61ad3759f06f","user-3f1cdf6632b8","collection-3fe99b2acc4"],"serverVariantState":"","middlewareEnabled":true,"cacheStatus":"DYNAMIC","shouldUseCache":false,"vary":[],"pubFeaturingPostPageLabelEnabled":false,"pubHierarchyEnabled":false},"client":{"hydrated":false,"isUs":false,"isNativeMedium":false,"isSafariMobile":false,"isSafari":false,"isFirefox":false,"routingEntity":{"type":"COLLECTION","id":"3fe99b2acc4","explicit":true},"viewerIsBot":false},"debug":{"requestId":"cd1367f2-bcbd-4532-9c7a-cd5197b49d11","requestTag":"","hybridDevServices":[],"originalSpanCarrier":{"traceparent":"00-bc1a2410a273970ccd388848f7851ce9-70de5b06dc2c137b-01"}},"multiVote":{"clapsPerPost":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Fai.gopubby.com\u002Fmicrosofts-graphrag-autogen-ollama-chainlit-fully-local-free-multi-agent-rag-superbot-61ad3759f06f","host":"ai.gopubby.com","hostname":"ai.gopubby.com","referrer":"","hasSetReferrer":false,"susiModal":{"step":null,"operation":"register"},"postRead":false,"partnerProgram":{"selectedCountryCode":null}},"config":{"nodeEnv":"production","version":"main-20250224-082317-7f65a8da54","target":"production","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","recaptchaEnterpriseKeyId":"6Le-uGgpAAAAAPprRaokM8AKthQ9KNGdoxaGUvVp","datadog":{"applicationId":"6702d87d-a7e0-42fe-bbcb-95b469547ea0","clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","rumToken":"pubf9cc52896502b9413b68ba36fc0c7162","context":{"deployment":{"target":"production","tag":"main-20250224-082317-7f65a8da54","commit":"7f65a8da544c83f37f80d832183f3b4587a663c4"}},"datacenter":"us"},"googleAnalyticsCode":"G-7JY7T788PK","googlePay":{"apiVersion":"2","apiVersionMinor":"0","merchantId":"BCR2DN6TV7EMTGBM","merchantName":"Medium","instanceMerchantId":"13685562959212738550"},"applePay":{"version":3},"signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumMastodonDomainName":"me.dm","mediumOwnedAndOperatedCollectionIds":["8a9336e5bb4","b7e45b22fec3","193b68bd4fba","8d6b8a439e32","54c98c43354d","3f6ecf56618","d944778ce714","92d2092dc598","ae2a65f35510","1285ba81cada","544c7006046e","fc8964313712","40187e704f1c","88d9857e584e","7b6769f2748b","bcc38c8f6edf","cef6983b292","cb8577c9149e","444d13b52878","713d7dbc99b0","ef8e90590e66","191186aaafa0","55760f21cdc5","9dc80918cc93","bdc4052bbdba","8ccfed20cbb2"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"topicsToFollow":["d61cf867d93f","8a146bc21b28","1eca0103fff3","4d562ee63426","aef1078a3ef5","e15e46793f8d","6158eb913466","55f1c20aba7a","3d18b94f6858","4861fee224fd","63c6f1f93ee","1d98b3a9a871","decb52b64abf","ae5d4995e225","830cded25262"],"topicToTagMappings":{"accessibility":"accessibility","addiction":"addiction","android-development":"android-development","art":"art","artificial-intelligence":"artificial-intelligence","astrology":"astrology","basic-income":"basic-income","beauty":"beauty","biotech":"biotech","blockchain":"blockchain","books":"books","business":"business","cannabis":"cannabis","cities":"cities","climate-change":"climate-change","comics":"comics","coronavirus":"coronavirus","creativity":"creativity","cryptocurrency":"cryptocurrency","culture":"culture","cybersecurity":"cybersecurity","data-science":"data-science","design":"design","digital-life":"digital-life","disability":"disability","economy":"economy","education":"education","equality":"equality","family":"family","feminism":"feminism","fiction":"fiction","film":"film","fitness":"fitness","food":"food","freelancing":"freelancing","future":"future","gadgets":"gadgets","gaming":"gaming","gun-control":"gun-control","health":"health","history":"history","humor":"humor","immigration":"immigration","ios-development":"ios-development","javascript":"javascript","justice":"justice","language":"language","leadership":"leadership","lgbtqia":"lgbtqia","lifestyle":"lifestyle","machine-learning":"machine-learning","makers":"makers","marketing":"marketing","math":"math","media":"media","mental-health":"mental-health","mindfulness":"mindfulness","money":"money","music":"music","neuroscience":"neuroscience","nonfiction":"nonfiction","outdoors":"outdoors","parenting":"parenting","pets":"pets","philosophy":"philosophy","photography":"photography","podcasts":"podcast","poetry":"poetry","politics":"politics","privacy":"privacy","product-management":"product-management","productivity":"productivity","programming":"programming","psychedelics":"psychedelics","psychology":"psychology","race":"race","relationships":"relationships","religion":"religion","remote-work":"remote-work","san-francisco":"san-francisco","science":"science","self":"self","self-driving-cars":"self-driving-cars","sexuality":"sexuality","social-media":"social-media","society":"society","software-engineering":"software-engineering","space":"space","spirituality":"spirituality","sports":"sports","startups":"startup","style":"style","technology":"technology","transportation":"transportation","travel":"travel","true-crime":"true-crime","tv":"tv","ux":"ux","venture-capital":"venture-capital","visual-design":"visual-design","work":"work","world":"world","writing":"writing"},"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"7*V1_7XP4snlmqrc_0Njontw.png","height":110,"width":500},"postLogo":{"imageId":"bd978bb536350a710e8efb012513429cabdc4c28700604261aeda246d0f980b7","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":[],"COVID_APPLICABLE_TOPIC_NAMES":[],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":[],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"unbound":{"text":"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":45,"end":59,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":127,"end":134,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"sharedVoteMessaging":{"TAGS":["politics","election-2020","government","us-politics","election","2020-presidential-race","trump","donald-trump","democrats","republicans","congress","republican-party","democratic-party","biden","joe-biden","maga"],"TOPICS":["politics","election"],"MESSAGE":{"text":"Find out more about the U.S. election results here.","markups":[{"start":46,"end":50,"href":"https:\u002F\u002Fcookpolitical.com\u002F2020-national-popular-vote-tracker"}]},"EXCLUDE_POSTS":["397ef29e3ca5"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4","braintree":{"enabled":true,"merchantId":"m56f8fqpf7ngnrd4","merchantAccountId":{"usd":"AMediumCorporation_instant","eur":"amediumcorporation_EUR","cad":"amediumcorporation_CAD"},"publicKey":"ds2nn34bg2z7j5gd","braintreeEnvironment":"production","dashboardUrl":"https:\u002F\u002Fwww.braintreegateway.com\u002Fmerchants","gracePeriodDurationInDays":14,"mediumMembershipPlanId":{"monthly":"ce105f8c57a3","monthlyV2":"e8a5e126-792b-4ee6-8fba-d574c1b02fc5","monthlyWithTrial":"d5ee3dbe3db8","monthlyPremium":"fa741a9b47a2","yearly":"a40ad4a43185","yearlyV2":"3815d7d6-b8ca-4224-9b8c-182f9047866e","yearlyStaff":"d74fb811198a","yearlyWithTrial":"b3bc7350e5c7","yearlyPremium":"e21bd2c12166","monthlyOneYearFree":"e6c0637a-2bad-4171-ab4f-3c268633d83c","monthly25PercentOffFirstYear":"235ecc62-0cdb-49ae-9378-726cd21c504b","monthly20PercentOffFirstYear":"ba518864-9c13-4a99-91ca-411bf0cac756","monthly15PercentOffFirstYear":"594c029b-9f89-43d5-88f8-8173af4e070e","monthly10PercentOffFirstYear":"c6c7bc9a-40f2-4b51-8126-e28511d5bdb0","monthlyForStudents":"629ebe51-da7d-41fd-8293-34cd2f2030a8","yearlyOneYearFree":"78ba7be9-0d9f-4ece-aa3e-b54b826f2bf1","yearly25PercentOffFirstYear":"2dbb010d-bb8f-4eeb-ad5c-a08509f42d34","yearly20PercentOffFirstYear":"47565488-435b-47f8-bf93-40d5fbe0ebc8","yearly15PercentOffFirstYear":"8259809b-0881-47d9-acf7-6c001c7f720f","yearly10PercentOffFirstYear":"9dd694fb-96e1-472c-8d9e-3c868d5c1506","yearlyForStudents":"e29345ef-ab1c-4234-95c5-70e50fe6bc23","monthlyCad":"p52orjkaceei","yearlyCad":"h4q9g2up9ktt"},"braintreeDiscountId":{"oneMonthFree":"MONTHS_FREE_01","threeMonthsFree":"MONTHS_FREE_03","sixMonthsFree":"MONTHS_FREE_06","fiftyPercentOffOneYear":"FIFTY_PERCENT_OFF_ONE_YEAR"},"3DSecureVersion":"2","defaultCurrency":"usd","providerPlanIdCurrency":{"4ycw":"usd","rz3b":"usd","3kqm":"usd","jzw6":"usd","c2q2":"usd","nnsw":"usd","q8qw":"usd","d9y6":"usd","fx7w":"cad","nwf2":"cad"}},"paypalClientId":"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v","paypal":{"host":"https:\u002F\u002Fapi.paypal.com:443","clientMode":"production","serverMode":"live","webhookId":"4G466076A0294510S","monthlyPlan":{"planId":"P-9WR0658853113943TMU5FDQA","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlan":{"planId":"P-7N8963881P8875835MU5JOPQ","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oneYearGift":{"name":"Medium Membership (1 Year, Digital Gift Code)","description":"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com\u002Fredeem.","price":"50.00","currency":"USD","sku":"membership-gift-1-yr"},"oldMonthlyPlan":{"planId":"P-96U02458LM656772MJZUVH2Y","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlan":{"planId":"P-59P80963JF186412JJZU3SMI","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"monthlyPlanWithTrial":{"planId":"P-66C21969LR178604GJPVKUKY","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlanWithTrial":{"planId":"P-6XW32684EX226940VKCT2MFA","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oldMonthlyPlanNoSetupFee":{"planId":"P-4N046520HR188054PCJC7LJI","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlanNoSetupFee":{"planId":"P-7A4913502Y5181304CJEJMXQ","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"sdkUrl":"https:\u002F\u002Fwww.paypal.com\u002Fsdk\u002Fjs"},"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","log":{"json":true,"level":"info"},"imageUploadMaxSizeMb":25,"staffPicks":{"title":"Staff Picks","catalogId":"c7bc6e1ee00f"}},"session":{"xsrf":"a99ee8b45f9c"}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","variantFlags":[{"__typename":"VariantFlag","name":"enable_legacy_feed_in_iceland","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_publish_permission_check","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_trust_service_recaptcha","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_sign_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_cancellation_discount_v1_1","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_user_follows","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_creator_welcome_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_conversion_ranker_v2","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"enable_footer_app_buttons","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_medium2_kbfd","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_trial_membership","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_cache_less_following_feed","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_engagement_service_publish_response","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_starspace","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_signup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_apple_pay","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_seamless_social_sharing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_intrinsic_automatic_actions","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_plans_page_payment_form","valueType":{"__typename":"VariantFlagString","value":"group_1"}},{"__typename":"VariantFlag","name":"enable_aurora_pub_follower_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_manage_membership_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_speechify_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_susi_redesign_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_dynamic_aspirational_paywall","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_configure_pronouns","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pp_country_expansion","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_cards_byline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_updated_pub_recs_ui","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_sms_verification_for_publish","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"signin_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"enable_braintree_paypal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_boost_experiment","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"enable_iceland_forced_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"reader_fair_distribution_non_qp","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"skip_fs_cache_user_vals","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_friend_links_postpage_banners","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_moc_load_processor_c","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_archive_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_diversification_rex","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_maim_the_meter","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_bottom_responses_native","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_cards","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_auto_follow_on_subscribe","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_premium_tier","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_medium_com_canonical_urls","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_winback_promotion_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_annual_plan","valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"}},{"__typename":"VariantFlag","name":"enable_braintree_client","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_one_tap","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_abandoned_paywall_promotion_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mastodon_avatar_upload","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_reading_history","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_deprecate_legacy_providers_v3","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_friend_links_creation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_remove_twitter_onboarding_step","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_social_share_sheet","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_conversion_model_v2","valueType":{"__typename":"VariantFlagString","value":"group_2"}},{"__typename":"VariantFlag","name":"enable_newsletter_lo_flow_custom_domains","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"price_smoke_test_yearly","valueType":{"__typename":"VariantFlagString","value":""}},{"__typename":"VariantFlag","name":"enable_ios_offline_reading","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_moc_load_processor_first_story","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pub_featuring_post_page_label","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rito_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_update_topic_portals_wtf","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"glyph_font_set","valueType":{"__typename":"VariantFlagString","value":"m2-unbound-source-serif-pro"}},{"__typename":"VariantFlag","name":"enable_lite_homepage","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_miro_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_plans_page_branding_v2","valueType":{"__typename":"VariantFlagString","value":"group_2"}},{"__typename":"VariantFlag","name":"enable_see_pronouns","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tribute_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_rating_prompt_stories_read_threshold","valueType":{"__typename":"VariantFlagNumber","value":2}},{"__typename":"VariantFlag","name":"enable_android_dynamic_programming_paywall","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_home_post_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_simplified_digest_v2_b","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_offline_reading","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mastodon_for_members","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_image_sharer","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tick_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"rex_generator_max_candidates","valueType":{"__typename":"VariantFlagNumber","value":1000}},{"__typename":"VariantFlag","name":"enable_sharer_validate_post_share_key","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"coronavirus_topic_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_moc_load_processor_all_recs_surfaces","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_switch_plan_premium_tier","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tipping_v0_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_verified_book_author","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_verified_author","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_autorefresh","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pub_featuring_notifications","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_abandoned_cart_promotion_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"onboarding_tags_from_top_views","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_syntax_highlight","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_marketing_emails","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_bottom_responses_input","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_sharer_create_post_share_key","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"price_smoke_test_monthly","valueType":{"__typename":"VariantFlagString","value":""}},{"__typename":"VariantFlag","name":"enable_recommended_publishers_query","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_verifications_service","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"get_highlights_from_engagement","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"mobile_custom_app_icon","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_cancellation_discount_v1_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_integration","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_aggregator_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"can_send_tips_v0","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_monthly_plan","valueType":{"__typename":"VariantFlagString","value":"60e220181034"}},{"__typename":"VariantFlag","name":"enable_automod","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_entities_to_follow_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_server_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_new_push_notification_endpoint","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_test_auth","valueType":{"__typename":"VariantFlagString","value":"disallow"}},{"__typename":"VariantFlag","name":"num_post_bottom_responses_to_show","valueType":{"__typename":"VariantFlagNumber","value":3}},{"__typename":"VariantFlag","name":"enable_ios_easy_resubscribe","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_recaptcha_enterprise","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_in_app_free_trial","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_dynamic_paywall_programming","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_stripe_customers","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tipping_v0_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_lists_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_members_only_audio","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_display_paywall_after_onboarding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lo_homepage","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"enable_mastodon_for_members_username_selection","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"redefined_top_posts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_two_hour_refresh","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_import","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_premium_tier_badge","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"rex_enable_presentation_filtering_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_google_pay","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_dynamic_paywall_aspiriational","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_editor_new_publishing_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_custom_moc_preview_for_google_referrer","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_annual_premium_plan","valueType":{"__typename":"VariantFlagString","value":"4a442ace1476"}},{"__typename":"VariantFlag","name":"enable_hybrid_ranking_model","valueType":{"__typename":"VariantFlagString","value":"experiment"}},{"__typename":"VariantFlag","name":"enable_update_explore_wtf","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"textshots_userid","valueType":{"__typename":"VariantFlagString","value":""}},{"__typename":"VariantFlag","name":"enable_app_flirty_thirty","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"reengagement_notification_duration","valueType":{"__typename":"VariantFlagNumber","value":3}},{"__typename":"VariantFlag","name":"enable_inline_comments","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pub_featuring","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"signup_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"ios_enable_friend_links_postpage_banners","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_topic_portals","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_speechify_widget","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tag_recs","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_post_referrers","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_group_gifting","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ranker_v10","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"enable_boost_nia_v01","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_deviant_get_variant_flag_from_medium2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_bottom_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pp_v4","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_iceland_nux","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"can_receive_tips_v0","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_continue_this_thread","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_response_markup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pub_featuring_stats","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_partner_program_enrollment","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_sprig","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_eventstats_event_processing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"goliath_externalsearch_enable_comment_deindexation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_friend_links_creation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_monthly_premium_plan","valueType":{"__typename":"VariantFlagString","value":"12a660186432"}},{"__typename":"VariantFlag","name":"enable_branch_io","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ml_rank_rex_anno","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_access","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"browsable_stream_config_bucket","valueType":{"__typename":"VariantFlagString","value":"curated-topics"}},{"__typename":"VariantFlag","name":"enable_pre_pp_v4","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_abandoned_paywall_email_experiment","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"enable_bg_post_post","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pill_based_home_feed","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_susi_redesign_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_recirc_model","valueType":{"__typename":"VariantFlagBoolean","value":true}}],"viewer":{"__ref":"User:72a203aea390"},"collectionByDomainOrSlug({\"domainOrSlug\":\"ai.gopubby.com\"})":{"__ref":"Collection:3fe99b2acc4"},"postResult({\"id\":\"61ad3759f06f\"})":{"__ref":"Post:61ad3759f06f"}},"UserViewerEdge:userId:72a203aea390-viewerId:72a203aea390":{"__typename":"UserViewerEdge","id":"userId:72a203aea390-viewerId:72a203aea390","createdAt":1555934178010},"User:72a203aea390":{"__typename":"User","id":"72a203aea390","allowEmailAddressSharingEditorWriter":true,"dismissableFlags":["WRITER_SUBSCRIPTIONS_TOOLTIP","USER_SIGNAL_MENU","AURORA_NAV_AVAILABLE"],"emailObfuscated":"gl•••••@gmail.com","geolocation":{"__typename":"Geolocation","country":"AT"},"hasGroupGiftingEnabled":false,"hasPastMemberships":true,"hasSubdomain":false,"imageId":"0*L8-gaiBv3nAhl_9Y.jpg","isEligibleToImportEmails":false,"isEligibleToViewNewResponses":true,"isMembershipTrialEligible":false,"isSuspended":false,"membership":null,"name":"Виталий","partnerProgramEnrollment":null,"postSubscribeMembershipUpsellShownAt":0,"styleEditorOnboardingVersionSeen":0,"twitterScreenName":"","unverifiedEmail":"","username":"glvv232","viewerEdge":{"__ref":"UserViewerEdge:userId:72a203aea390-viewerId:72a203aea390"},"pronouns":[]},"ImageMetadata:1*8xkFb5PlJ9-jlzImmpZsGg.png":{"__typename":"ImageMetadata","id":"1*8xkFb5PlJ9-jlzImmpZsGg.png"},"Collection:3fe99b2acc4":{"__typename":"Collection","id":"3fe99b2acc4","favicon":{"__ref":"ImageMetadata:1*8xkFb5PlJ9-jlzImmpZsGg.png"},"customStyleSheet":null,"colorPalette":{"__typename":"ColorPalette","highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFECF3FF","point":0},{"__typename":"ColorPoint","color":"#FFE8F2FF","point":0.1},{"__typename":"ColorPoint","color":"#FFE4F0FF","point":0.2},{"__typename":"ColorPoint","color":"#FFE1EFFF","point":0.3},{"__typename":"ColorPoint","color":"#FFDDEDFF","point":0.4},{"__typename":"ColorPoint","color":"#FFD9EBFF","point":0.5},{"__typename":"ColorPoint","color":"#FFD5EAFF","point":0.6},{"__typename":"ColorPoint","color":"#FFD1E8FF","point":0.7},{"__typename":"ColorPoint","color":"#FFCDE6FF","point":0.8},{"__typename":"ColorPoint","color":"#FFC9E5FF","point":0.9},{"__typename":"ColorPoint","color":"#FFC5E3FF","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF6288C1","point":0},{"__typename":"ColorPoint","color":"#FF5C7EB1","point":0.1},{"__typename":"ColorPoint","color":"#FF5775A2","point":0.2},{"__typename":"ColorPoint","color":"#FF506B93","point":0.3},{"__typename":"ColorPoint","color":"#FF4A6183","point":0.4},{"__typename":"ColorPoint","color":"#FF435674","point":0.5},{"__typename":"ColorPoint","color":"#FF3B4B64","point":0.6},{"__typename":"ColorPoint","color":"#FF334054","point":0.7},{"__typename":"ColorPoint","color":"#FF2A3444","point":0.8},{"__typename":"ColorPoint","color":"#FF202834","point":0.9},{"__typename":"ColorPoint","color":"#FF151B23","point":1}]},"tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF264C82","colorPoints":[{"__typename":"ColorPoint","color":"#FF264C82","point":0},{"__typename":"ColorPoint","color":"#FF416294","point":0.1},{"__typename":"ColorPoint","color":"#FF5877A4","point":0.2},{"__typename":"ColorPoint","color":"#FF6F8AB3","point":0.3},{"__typename":"ColorPoint","color":"#FF849CC2","point":0.4},{"__typename":"ColorPoint","color":"#FF99AECF","point":0.5},{"__typename":"ColorPoint","color":"#FFADBFDC","point":0.6},{"__typename":"ColorPoint","color":"#FFC1D0E9","point":0.7},{"__typename":"ColorPoint","color":"#FFD4E0F5","point":0.8},{"__typename":"ColorPoint","color":"#FFE7F0FF","point":0.9},{"__typename":"ColorPoint","color":"#FFFAFFFF","point":1}]}},"domain":"ai.gopubby.com","slug":"ai-advances","googleAnalyticsId":null,"name":"AI Advances","avatar":{"__ref":"ImageMetadata:1*R8zEd59FDf0l8Re94ImV0Q.png"},"description":"Democratizing access to artificial intelligence","subscriberCount":23193,"latestPostsConnection({\"paging\":{\"limit\":1}})":{"__typename":"PostConnection","posts":[{"__ref":"Post:2e3473bc6f46"}]},"isAuroraVisible":false,"tintColor":"#FF264C82","newsletterV3":{"__ref":"NewsletterV3:e9407e0f697f"},"viewerEdge":{"__ref":"CollectionViewerEdge:collectionId:3fe99b2acc4-viewerId:72a203aea390"},"twitterUsername":"AIAdvances","facebookPageId":null,"logo":{"__ref":"ImageMetadata:"}},"ImageMetadata:1*R8zEd59FDf0l8Re94ImV0Q.png":{"__typename":"ImageMetadata","id":"1*R8zEd59FDf0l8Re94ImV0Q.png"},"User:ceee7ce3cb":{"__typename":"User","id":"ceee7ce3cb","customDomainState":null,"hasSubdomain":false,"username":"souvik.0204"},"Post:2e3473bc6f46":{"__typename":"Post","id":"2e3473bc6f46","firstPublishedAt":1740338414727,"creator":{"__ref":"User:ceee7ce3cb"},"collection":{"__ref":"Collection:3fe99b2acc4"},"isSeries":false,"mediumUrl":"https:\u002F\u002Fai.gopubby.com\u002Fthe-ai-race-everyones-getting-wrong-2e3473bc6f46","sequence":null,"uniqueSlug":"the-ai-race-everyones-getting-wrong-2e3473bc6f46"},"NewsletterV3:e9407e0f697f":{"__typename":"NewsletterV3","id":"e9407e0f697f"},"LinkedAccounts:3f1cdf6632b8":{"__typename":"LinkedAccounts","mastodon":null,"id":"3f1cdf6632b8"},"Membership:e26841474eaa":{"__typename":"Membership","tier":"MEMBER","id":"e26841474eaa"},"User:3f1cdf6632b8":{"__typename":"User","id":"3f1cdf6632b8","linkedAccounts":{"__ref":"LinkedAccounts:3f1cdf6632b8"},"isSuspended":false,"name":"Karthik Rajan, Ph.D","imageId":"1*mGw0B5wtYwJDrrVhxaL2NA.jpeg","customDomainState":null,"hasSubdomain":false,"username":"karthikrajanv","verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"socialStats":{"__typename":"SocialStats","followerCount":970,"followingCount":8,"collectionFollowingCount":2},"bio":"I explore, develop tools & write about things at the intersection of computational mechanics, material science, engineering, & generative AI.","membership":{"__ref":"Membership:e26841474eaa"},"allowNotes":true,"viewerEdge":{"__ref":"UserViewerEdge:userId:3f1cdf6632b8-viewerId:72a203aea390"},"twitterScreenName":""},"Paragraph:55921d74a83c_0":{"__typename":"Paragraph","id":"55921d74a83c_0","name":"bbf2","type":"H3","href":null,"layout":null,"metadata":null,"text":"Microsoft’s GraphRAG + AutoGen + Ollama + Chainlit = Local & Free Multi-Agent RAG Superbot","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_1":{"__typename":"Paragraph","id":"55921d74a83c_1","name":"5af4","type":"H4","href":null,"layout":null,"metadata":null,"text":"Guide to constructing an agentic-GraphRAG retrieval application—all codes in my GitHub repo.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*2ePkbvke5ObCuJu-g2pzsg.png":{"__typename":"ImageMetadata","id":"1*2ePkbvke5ObCuJu-g2pzsg.png","originalHeight":2336,"originalWidth":3072,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:55921d74a83c_2":{"__typename":"Paragraph","id":"55921d74a83c_2","name":"98cf","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*2ePkbvke5ObCuJu-g2pzsg.png"},"text":"Graphical abstract of the integration and key components","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_3":{"__typename":"Paragraph","id":"55921d74a83c_3","name":"de28","type":"P","href":null,"layout":null,"metadata":null,"text":"Retrieval-augmented generation (RAG) is a powerful tool that equips large language models (LLMs) with the ability to access real-world data for more informed responses. This is achieved by integrating the models with a vector database for real-time learning and adaptation. This feature makes RAG a preferred choice for applications such as chatbots and virtual assistants, where the demand for accurate and sensible responses in real time is high. An advanced variant of this, known as Graph Retrieval-Augmented Generation (GraphRAG), merges the benefits of graph-based knowledge retrieval with LLMs, further enhancing the capabilities in natural language processing. Unlike traditional RAG methods that rely on vector similarity searches, GraphRAG constructs a structured knowledge graph from raw text, capturing entities, relationships, and critical claims. This can enhance LLMs’ ability to understand and synthesize complex datasets and their relationships, yielding more accurate and contextually grounded responses.","hasDropCap":true,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*zhdPp80vsgQcl6c0Pn_AMA.png":{"__typename":"ImageMetadata","id":"1*zhdPp80vsgQcl6c0Pn_AMA.png","originalHeight":405,"originalWidth":810,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:55921d74a83c_4":{"__typename":"Paragraph","id":"55921d74a83c_4","name":"00ac","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*zhdPp80vsgQcl6c0Pn_AMA.png"},"text":"Extracted from a paper by Markus Beuhler from MIT (link here)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":56,"end":60,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2403.11996","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_5":{"__typename":"Paragraph","id":"55921d74a83c_5","name":"f5c6","type":"P","href":null,"layout":null,"metadata":null,"text":"AutoGen is a tool by Microsoft that streamlines the development of intricate applications based on multi-agent LLMs by automating and optimizing workflows that were once complicated and required significant manual effort. Picture AutoGen as a platform where you can interact with multiple GPTs instead of just one. Each GPT acts as an individual “agent”, playing a unique part in a comprehensive operation. Combining GraphRAG’s retrieval strengths with AutoGen AI agents’ conversational and task-oriented functionalities results in robust AI assistants capable of efficiently handling detailed queries, generating and executing codes, creating multi-page scientific reports, and conducting data analysis. Furthermore, offline local LLMs, such as those from Ollama or LM Studio, ensure cost-effective and secure data processing. Local LLMs eliminate the high costs and privacy risks associated with online LLMs, keeping sensitive data within the organization and reducing operational expenses.","hasDropCap":true,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_6":{"__typename":"Paragraph","id":"55921d74a83c_6","name":"fe35","type":"BQ","href":null,"layout":null,"metadata":null,"text":"This article will guide you on constructing a multi-agent AI application with GraphRAG retrieval system, which operates entirely on your local machine and is available at no charge. Here are the key components of this application:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_7":{"__typename":"Paragraph","id":"55921d74a83c_7","name":"c46e","type":"OLI","href":null,"layout":null,"metadata":null,"text":"GraphRAG’s knowledge search methods are integrated with an AutoGen agent via function calling.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_8":{"__typename":"Paragraph","id":"55921d74a83c_8","name":"8961","type":"OLI","href":null,"layout":null,"metadata":null,"text":"GraphRAG (local & global search) is configured to support local models from Ollama for inference and embedding.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_9":{"__typename":"Paragraph","id":"55921d74a83c_9","name":"904f","type":"OLI","href":null,"layout":null,"metadata":null,"text":"AutoGen was extended to support function calling with non-OpenAI LLMs from Ollama via the Lite-LLM proxy server.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_10":{"__typename":"Paragraph","id":"55921d74a83c_10","name":"018f","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Chainlit UI to handle continuous conversations, multi-threading, and user input settings.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_11":{"__typename":"Paragraph","id":"55921d74a83c_11","name":"acbc","type":"P","href":null,"layout":null,"metadata":null,"text":"Given my material science and computational modeling background, I wanted to test this application by constructing knowledge graphs from documentation of ABAQUS, an FEA engineering software, and some technical data sheets of carbon fibers and polymers. The overall accuracy of using the local LLMs could be better, considering the complexity of this dataset. Future articles will explore learnings from benchmark studies using different models for embedding and inference. Nevertheless, I am eager to build more complex knowledge graphs from scientific journals and data in this field, test advanced engineering code generation tasks, and utilize a conversational assistant to brainstorm scientific topics within my expertise. The application looks like this.","hasDropCap":true,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*Emn2loXpcEHw67-mYtASzg.jpeg":{"__typename":"ImageMetadata","id":"1*Emn2loXpcEHw67-mYtASzg.jpeg","originalHeight":1927,"originalWidth":1512,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:55921d74a83c_12":{"__typename":"Paragraph","id":"55921d74a83c_12","name":"b446","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*Emn2loXpcEHw67-mYtASzg.jpeg"},"text":"Main application UI with example queries. The last two have the same query, but the first is a global search, while the second is a local one.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*0Wp0YSLDQsp05W_lycl8vQ.png":{"__typename":"ImageMetadata","id":"1*0Wp0YSLDQsp05W_lycl8vQ.png","originalHeight":444,"originalWidth":423,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:55921d74a83c_13":{"__typename":"Paragraph","id":"55921d74a83c_13","name":"5afc","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*0Wp0YSLDQsp05W_lycl8vQ.png"},"text":"Widget settings to switch between local and global search, set community levels, and generation length.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_14":{"__typename":"Paragraph","id":"55921d74a83c_14","name":"e685","type":"P","href":null,"layout":null,"metadata":null,"text":"The development was done in a Linux environment using the Windows Subsystem for Linux (WSL) and Visual Studio Code on a Windows 11 PC with an i9 13th Gen processor, 64 GB RAM, and 24 GB Nvidia RTX 4090. For the best experience in developing and testing this app, it is recommended to use a Linux distribution or WSL. I have not tested this on a native Windows environment. For guidelines on installing WSL and setting up Python and Conda environments, please refer to this article (here). Additional references and relevant information are provided at the end of this article.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":482,"end":486,"href":"https:\u002F\u002Frobkerr.ai\u002Ffine-tuning-llms-using-a-local-gpu-on-windows\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_15":{"__typename":"Paragraph","id":"55921d74a83c_15","name":"0b3a","type":"P","href":null,"layout":null,"metadata":null,"text":"Here is the link to the source code repository. Now, let’s get started!!","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":12,"end":16,"href":"https:\u002F\u002Fgithub.com\u002Fkarthik-codex\u002Fautogen_graphRAG","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_16":{"__typename":"Paragraph","id":"55921d74a83c_16","name":"877f","type":"H3","href":null,"layout":null,"metadata":null,"text":"Install model dependencies & clone repository.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_17":{"__typename":"Paragraph","id":"55921d74a83c_17","name":"0299","type":"H4","href":null,"layout":null,"metadata":null,"text":"Install language models from Ollama for inference and embedding","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_18":{"__typename":"Paragraph","id":"55921d74a83c_18","name":"5c1c","type":"PRE","href":null,"layout":null,"metadata":null,"text":"# Mistral for GraphRAG Inference\nollama pull mistral\n\n# Nomic-Embed-Text for GraphRAG Embedding\nollama pull nomic-embed-text\n\n# LLama3 for Autogen Inference\nollama pull llama3\n\n# Host Ollama on a local server: http:\u002F\u002Flocalhost:11434\nollama serve","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"bash"},"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_19":{"__typename":"Paragraph","id":"55921d74a83c_19","name":"f007","type":"H4","href":null,"layout":null,"metadata":null,"text":"Create a conda environment and install these dependencies","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":57,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_20":{"__typename":"Paragraph","id":"55921d74a83c_20","name":"83b5","type":"PRE","href":null,"layout":null,"metadata":null,"text":"# Create and activate a conda environment\nconda create -n RAG_agents python=3.12\nconda activate RAG_agents\n\n# Lite-LLM proxy server for Ollama\npip install 'litellm[proxy]'\n\n# Install Ollama\npip install ollama\n\n# Microsoft AutoGen\npip install pyautogen \"pyautogen[retrievechat]\" \n\n# Microsoft GraphRAG\npip install graphrag\n\n# Text-Token Encoder-Decoder\npip install tiktoken\n\n# Chainlit Python application\npip install chainlit\n\n# Clone my Git-hub repository\ngit clone https:\u002F\u002Fgithub.com\u002Fkarthik-codex\u002Fautogen_graphRAG.git\n\n# (BONUS) To Convert PDF files to Markdown for GraphRAG \npip install marker-pdf\n\n# (BONUS) Only if you installed Marker-pdf since it removes GPU CUDA support by default\nconda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"bash"},"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_21":{"__typename":"Paragraph","id":"55921d74a83c_21","name":"9bf3","type":"P","href":null,"layout":null,"metadata":null,"text":"You will find the following files in my GitHub repository.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_22":{"__typename":"Paragraph","id":"55921d74a83c_22","name":"d0cb","type":"OLI","href":null,"layout":null,"metadata":null,"text":"\u002Frequirements.txt— Contains a list of all the above packages","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":0,"end":17,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":19,"end":60,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_23":{"__typename":"Paragraph","id":"55921d74a83c_23","name":"ced6","type":"OLI","href":null,"layout":null,"metadata":null,"text":"\u002Futils\u002Fsettings.yaml— Contains the LLM config for using Mistral 7B and Nomic-Text-Embedding from Ollama for GraphRAG offline embedding and indexing. You will use this file to replace the one created when you initialize GraphRAG in your working directory for the first time.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":0,"end":20,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":22,"end":148,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":149,"end":273,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_24":{"__typename":"Paragraph","id":"55921d74a83c_24","name":"c051","type":"OLI","href":null,"layout":null,"metadata":null,"text":"\u002Futils\u002Fchainlit_agents.py— Contains class definitions that include AutoGen’s assistant and user proxy agents. This allows multiple agents to be tracked and their messages displayed in the UI. (Shout out to the Chainlit team for building the template).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":0,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":241,"end":249,"href":"https:\u002F\u002Fgithub.com\u002FChainlit\u002Fcookbook\u002Fblob\u002Fmain\u002Fpyautogen","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":27,"end":251,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_25":{"__typename":"Paragraph","id":"55921d74a83c_25","name":"721d","type":"OLI","href":null,"layout":null,"metadata":null,"text":"\u002Futils\u002Fembedding.py—Contains the modified embedding functions for GraphRAG embedding for local search queries using Ollama. You will use this file to replace the one inside GraphRAG package (more info below)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":0,"end":19,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":20,"end":207,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_26":{"__typename":"Paragraph","id":"55921d74a83c_26","name":"952d","type":"OLI","href":null,"layout":null,"metadata":null,"text":"utils\u002Fopenai_embeddings_llm.py—Contains the modified embedding functions for GraphRAG indexing and embedding using Ollama. You will use this file to replace the one inside GraphRAG package (more info below).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":0,"end":30,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":32,"end":207,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_27":{"__typename":"Paragraph","id":"55921d74a83c_27","name":"2a1e","type":"OLI","href":null,"layout":null,"metadata":null,"text":"\u002FappUI.py—Contains the main asynchronous functions to set up agents, define GraphRAG search functions, track and handle messages, and display them inside Chainlit UI.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":10,"end":166,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_28":{"__typename":"Paragraph","id":"55921d74a83c_28","name":"995f","type":"OLI","href":null,"layout":null,"metadata":null,"text":"\u002Futils\u002Fpdf_to_markdown.py — Bonus file containing functions to convert PDF files to markdown files for GraphRAG ingestion.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":0,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":28,"end":122,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_29":{"__typename":"Paragraph","id":"55921d74a83c_29","name":"fc23","type":"H3","href":null,"layout":null,"metadata":null,"text":"Create a GraphRAG knowledge base.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_30":{"__typename":"Paragraph","id":"55921d74a83c_30","name":"57e7","type":"H4","href":null,"layout":null,"metadata":null,"text":"Initialize GraphRAG in the root folder of the repository","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":56,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_31":{"__typename":"Paragraph","id":"55921d74a83c_31","name":"5641","type":"PRE","href":null,"layout":null,"metadata":null,"text":"#make a new folder \"input\" to place your input files for GraphRAG (.txt or .md)\nmkdir -p .\u002Finput\n\n# Initialize GraphRAG to create the required files and folders in the root dir\npython -m graphrag.index --init  --root .\n\n# Move the settings.yaml file to replace the one created by GraphRAG --init\nmv .\u002Futils\u002Fsettings.yaml .\u002F","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"bash"},"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_32":{"__typename":"Paragraph","id":"55921d74a83c_32","name":"acf5","type":"H4","href":null,"layout":null,"metadata":null,"text":"Configure GraphRAG settings to support local models from Ollama","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":63,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_33":{"__typename":"Paragraph","id":"55921d74a83c_33","name":"9652","type":"P","href":null,"layout":null,"metadata":null,"text":"Below is a snippet from settings.yaml illustrating the configuration of LLMs for creating indexes and embeddings. GraphRAG requires a 32k context length for indexing, making Mistral the chosen model. For embeddings, Nomic-embed-text is selected, although you can experiment with other embeddings from Ollama. No need to set ${GRAPHRAG_API_KEY}, as access is not required to these local models’ endpoints.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":24,"end":37,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":324,"end":343,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_34":{"__typename":"Paragraph","id":"55921d74a83c_34","name":"3d54","type":"PRE","href":null,"layout":null,"metadata":null,"text":"encoding_model: cl100k_base\nskip_workflows: []\nllm:\n  api_key: ${GRAPHRAG_API_KEY}\n  type: openai_chat # or azure_openai_chat\n  model: mistral\n  model_supports_json: true\n  api_base: http:\u002F\u002Flocalhost:11434\u002Fv1 \n.\n.\n.\nembeddings:\n  async_mode: threaded # or asyncio\n  llm:\n    api_key: ${GRAPHRAG_API_KEY}\n    type: openai_embedding # or azure_openai_embedding\n    model: nomic_embed_text\n    api_base: http:\u002F\u002Flocalhost:11434\u002Fapi \n.\n.\n.\ninput:  #Change input file pattern to.md, or .txt\n  type: file # or blob\n  file_type: text # or csv\n  base_dir: \"input\"\n  file_encoding: utf-8\n  file_pattern: \".*\\\\.md$\"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"yaml"},"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_35":{"__typename":"Paragraph","id":"55921d74a83c_35","name":"8283","type":"P","href":null,"layout":null,"metadata":null,"text":"You can specify the folder containing the input files in the “input” folder in the root directory. Both text and markdown files can be used. You can use the \u002Futils\u002Fpdf_to_markdown.py to convert your PDFs to markdown files that are then placed inside the “input” folder. Handling multiple file formats has not been figured out, but it is a solvable issue.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":157,"end":182,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_36":{"__typename":"Paragraph","id":"55921d74a83c_36","name":"4437","type":"P","href":null,"layout":null,"metadata":null,"text":"Before running GraphRAG to index, create embeddings, and perform local queries, you must modify the Python files openai_embeddings_llm.py and embedding.pylocated within the GraphRAG package. Without this modification, GraphRAG will throw an error when creating embeddings, as it won't recognize \"nomic-embed-text\" as a valid embedding model from Ollama. In my setup, these files are located at \u002Fhome\u002Fkarthik\u002Fminiconda3\u002Fenvs\u002FRAG_agents\u002Flib\u002Fpython3.12\u002Fsite-packages\u002Fgraphrag\u002Fllm\u002Fopenai\u002Fopenai_embeddings_llm.pyand \u002Fhome\u002Fkarthik\u002Fminiconda3\u002Fenvs\u002FRAG_agents\u002Flib\u002Fpython3.12\u002Fsite-packages\u002Fgraphrag\u002Fquery\u002Fllm\u002Foai\u002Fembedding.py","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":113,"end":138,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":142,"end":154,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":394,"end":508,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":512,"end":617,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_37":{"__typename":"Paragraph","id":"55921d74a83c_37","name":"08e9","type":"P","href":null,"layout":null,"metadata":null,"text":"You can locate these files using the command sudo find \u002F -name openai_embeddings_llm.py.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":45,"end":87,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_38":{"__typename":"Paragraph","id":"55921d74a83c_38","name":"bae6","type":"H4","href":null,"layout":null,"metadata":null,"text":"Create embeddings and knowledge graphs.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_39":{"__typename":"Paragraph","id":"55921d74a83c_39","name":"f7a5","type":"P","href":null,"layout":null,"metadata":null,"text":"Lastly, we create the embeddings and test the knowledge graph using the global or local search method. After completing the embedding process, you can find the output artifacts (.parquet files) and reports (.json and .logs) in the “output” folder of your GraphRAG working directory, which is the root folder in this instance.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_40":{"__typename":"Paragraph","id":"55921d74a83c_40","name":"fd03","type":"PRE","href":null,"layout":null,"metadata":null,"text":"# Create knowledge graph - this takes some time\npython -m graphrag.index --root .\n\n# Test GraphRAG\npython -m graphrag.query --root . --method global \"\u003Cinsert your query\u003E\"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"bash"},"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_41":{"__typename":"Paragraph","id":"55921d74a83c_41","name":"664c","type":"H4","href":null,"layout":null,"metadata":null,"text":"Start the Lite-LLM server and run the app from the terminal","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_42":{"__typename":"Paragraph","id":"55921d74a83c_42","name":"7ce7","type":"P","href":null,"layout":null,"metadata":null,"text":"Below is the command to initialize the server before running the app. I chose Llama3:8b to test this app. You can use larger models if your hardware permits. More information on Lite-LLM can be found at this link. Now, you are ready to run the application from another terminal. Make sure you are in the right conda environment.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":208,"end":212,"href":"https:\u002F\u002Fmicrosoft.github.io\u002Fautogen-for-net\u002Farticles\u002FFunction-call-with-ollama-and-litellm.html","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_43":{"__typename":"Paragraph","id":"55921d74a83c_43","name":"78e4","type":"PRE","href":null,"layout":null,"metadata":null,"text":"# start server from terminal\nlitellm --model ollama_chat\u002Fllama3\n\n# run app from another terminal\nchainlit run appUI.py","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"bash"},"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_44":{"__typename":"Paragraph","id":"55921d74a83c_44","name":"6a6d","type":"H3","href":null,"layout":null,"metadata":null,"text":"Breakdown: Core components of appUI.py","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_45":{"__typename":"Paragraph","id":"55921d74a83c_45","name":"97ab","type":"H4","href":null,"layout":null,"metadata":null,"text":"Import python libraries","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_46":{"__typename":"Paragraph","id":"55921d74a83c_46","name":"1aea","type":"PRE","href":null,"layout":null,"metadata":null,"text":"import autogen\nfrom rich import print\nimport chainlit as cl\nfrom typing_extensions import Annotated\nfrom chainlit.input_widget import (\n   Select, Slider, Switch)\nfrom autogen import AssistantAgent, UserProxyAgent\nfrom utils.chainlit_agents import ChainlitUserProxyAgent, ChainlitAssistantAgent\nfrom graphrag.query.cli import run_global_search, run_local_search","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_47":{"__typename":"Paragraph","id":"55921d74a83c_47","name":"36d1","type":"P","href":null,"layout":null,"metadata":null,"text":"You will notice two classes being imported from chainlit_agents. These wrapper classes for AutoGen agents enable Chainlit to track their conversations and handle termination or other user inputs. You can read more about this here.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":225,"end":229,"href":"https:\u002F\u002Fmedium.com\u002F@antoineross\u002Fautogen-web-application-using-chainlit-8c5ebf5a4e75","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":48,"end":63,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_48":{"__typename":"Paragraph","id":"55921d74a83c_48","name":"359d","type":"H4","href":null,"layout":null,"metadata":null,"text":"Configure AutoGen agents","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_49":{"__typename":"Paragraph","id":"55921d74a83c_49","name":"760d","type":"P","href":null,"layout":null,"metadata":null,"text":"The AutoGen agents utilize models from Ollama via the Lite-LLM proxy server. This is necessary because AutoGen does not support function calling through non-OpenAI inference models. The proxy server enables using Ollama models for function calling and code execution.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_50":{"__typename":"Paragraph","id":"55921d74a83c_50","name":"4f90","type":"PRE","href":null,"layout":null,"metadata":null,"text":"# LLama3 LLM from Lite-LLM Server for Agents #\nllm_config_autogen = {\n    \"seed\": 40,  # change the seed for different trials\n    \"temperature\": 0,\n    \"config_list\": [{\"model\": \"litellm\", \n                     \"base_url\": \"http:\u002F\u002F0.0.0.0:4000\u002F\", \n                     'api_key': 'ollama'},\n    ],\n    \"timeout\": 60000,\n}","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_51":{"__typename":"Paragraph","id":"55921d74a83c_51","name":"bb59","type":"H4","href":null,"layout":null,"metadata":null,"text":"Instantiate agents and input user settings at the start of the chat","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_52":{"__typename":"Paragraph","id":"55921d74a83c_52","name":"3a3f","type":"P","href":null,"layout":null,"metadata":null,"text":"I created three Chainlit widgets (switch, select, and slider) as user settings to choose the GraphRAG search type, community level, and content generation type. When turned ON, the switch widget uses the GraphRAG local search method for querying. The select options for content generation include “prioritized list,” “single paragraph,” “multiple paragraphs,” and “multiple-page report.” The slider widget selects the community generation level with options 0, 1, and 2. You can read more about the GraphRAG communities here.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":520,"end":524,"href":"https:\u002F\u002Fmlnotes.substack.com\u002Fp\u002Fgraphrag-combining-knowledge-graphs","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_53":{"__typename":"Paragraph","id":"55921d74a83c_53","name":"9be9","type":"PRE","href":null,"layout":null,"metadata":null,"text":"@cl.on_chat_start\nasync def on_chat_start():\n  try:\n    settings = await cl.ChatSettings(\n            [      \n                Switch(id=\"Search_type\", label=\"(GraphRAG) Local Search\", initial=True),       \n                Select(\n                    id=\"Gen_type\",\n                    label=\"(GraphRAG) Content Type\",\n                    values=[\"prioritized list\", \"single paragraph\", \"multiple paragraphs\", \"multiple-page report\"],\n                    initial_index=1,\n                ),          \n                Slider(\n                    id=\"Community\",\n                    label=\"(GraphRAG) Community Level\",\n                    initial=0,\n                    min=0,\n                    max=2,\n                    step=1,\n                ),\n\n            ]\n        ).send()\n\n    response_type = settings[\"Gen_type\"]\n    community = settings[\"Community\"]\n    local_search = settings[\"Search_type\"]\n    \n    cl.user_session.set(\"Gen_type\", response_type)\n    cl.user_session.set(\"Community\", community)\n    cl.user_session.set(\"Search_type\", local_search)\n\n    retriever   = AssistantAgent(\n       name=\"Retriever\", \n       llm_config=llm_config_autogen, \n       system_message=\"\"\"Only execute the function query_graphRAG to look for context. \n                    Output 'TERMINATE' when an answer has been provided.\"\"\",\n       max_consecutive_auto_reply=1,\n       human_input_mode=\"NEVER\", \n       description=\"Retriever Agent\"\n    )\n\n    user_proxy = ChainlitUserProxyAgent(\n        name=\"User_Proxy\",\n        human_input_mode=\"ALWAYS\",\n        llm_config=llm_config_autogen,\n        is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n        code_execution_config=False,\n        system_message='''A human admin. Interact with the retriever to provide any context''',\n        description=\"User Proxy Agent\"\n    )\n    \n    print(\"Set agents.\")\n\n    cl.user_session.set(\"Query Agent\", user_proxy)\n    cl.user_session.set(\"Retriever\", retriever)\n\n    msg = cl.Message(content=f\"\"\"Hello! What task would you like to get done today?      \n                     \"\"\", \n                     author=\"User_Proxy\")\n    await msg.send()\n\n    print(\"Message sent.\")\n    \n  except Exception as e:\n    print(\"Error: \", e)\n    pass","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_54":{"__typename":"Paragraph","id":"55921d74a83c_54","name":"ecec","type":"P","href":null,"layout":null,"metadata":null,"text":"I chose not to use the Chainlit wrapper class for the retriever assistant agent. This allowed me to disable tracking of the retriever’s output and directly capture the response from the GraphRAG function. The reason is that when the response passes through the retriever, the text loses its formatting, including spaces and paragraph indents. This issue was especially noticeable when generating multi-page reports with main and sub-headings. I could preserve the original formatting by bypassing the Chainlit wrapper and directly retrieving the output from the GraphRAG function. You will see how I achieved this below.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_55":{"__typename":"Paragraph","id":"55921d74a83c_55","name":"7e48","type":"H4","href":null,"layout":null,"metadata":null,"text":"Update changes in input settings","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_56":{"__typename":"Paragraph","id":"55921d74a83c_56","name":"edae","type":"P","href":null,"layout":null,"metadata":null,"text":"This function detects any changes made to the select, switch, and slider widgets from settings so it can reflect those changes in the subsequent queries.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_57":{"__typename":"Paragraph","id":"55921d74a83c_57","name":"921a","type":"PRE","href":null,"layout":null,"metadata":null,"text":"@cl.on_settings_update\nasync def setup_agent(settings):\n    response_type = settings[\"Gen_type\"]\n    community = settings[\"Community\"]\n    local_search = settings[\"Search_type\"]\n    cl.user_session.set(\"Gen_type\", response_type)\n    cl.user_session.set(\"Community\", community)\n    cl.user_session.set(\"Search_type\", local_search)\n    print(\"on_settings_update\", settings)","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_58":{"__typename":"Paragraph","id":"55921d74a83c_58","name":"5f6d","type":"H4","href":null,"layout":null,"metadata":null,"text":"Update UI with incoming messages from agents and the user.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_59":{"__typename":"Paragraph","id":"55921d74a83c_59","name":"4ea6","type":"P","href":null,"layout":null,"metadata":null,"text":"This is the core part of the application, which creates a group chat with two agents, defines a function “state_transition” to manage the conversation sequence, and provides the asynchronous RAG query function.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_60":{"__typename":"Paragraph","id":"55921d74a83c_60","name":"0a1a","type":"P","href":null,"layout":null,"metadata":null,"text":"You will notice INPUT_DIR ,ROOT_DIR, RESPONSE_TYPE, COMMUNTIY parameters that are passed into the local and global search GraphRAG query functions based on the bool parameterLOCAL_SEARCH. TheROOT_DIR, is set to ’.’ — pay attention to this if you initialized GraphRAG in a different directory.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":16,"end":36,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":37,"end":51,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":52,"end":61,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":174,"end":186,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":191,"end":200,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":211,"end":214,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_61":{"__typename":"Paragraph","id":"55921d74a83c_61","name":"edb0","type":"P","href":null,"layout":null,"metadata":null,"text":"The asynchronous function “query_graphRAG” calls the GraphRAG global or local search method. You will notice the line await cl.Message(content=result.response).send() inside the async def query_graphRAG function that directly retrieves the output from the RAG query and preserves the text formatting of the retrieved content.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":118,"end":166,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":178,"end":202,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_62":{"__typename":"Paragraph","id":"55921d74a83c_62","name":"61ea","type":"PRE","href":null,"layout":null,"metadata":null,"text":"@cl.on_message\nasync def run_conversation(message: cl.Message):\n    print(\"Running conversation\")\n    CONTEXT = message.content\n\n    MAX_ITER = 10\n    INPUT_DIR = None\n    ROOT_DIR = '.'\n    RESPONSE_TYPE = cl.user_session.get(\"Gen_type\")\n    COMMUNITY = cl.user_session.get(\"Community\")\n    LOCAL_SEARCH = cl.user_session.get(\"Search_type\")\n\n    print(\"Setting groupchat\")\n\n    retriever   = cl.user_session.get(\"Retriever\")\n    user_proxy  = cl.user_session.get(\"Query Agent\")\n\n    def state_transition(last_speaker, groupchat):\n        messages = groupchat.messages\n        if last_speaker is user_proxy:\n            return retriever\n        if last_speaker is retriever:\n            if messages[-1][\"content\"].lower() not in ['math_expert','physics_expert']:\n                return user_proxy\n            else:\n                if messages[-1][\"content\"].lower() == 'math_expert':\n                    return user_proxy\n                else:\n                    return user_proxy\n        else:\n            pass\n            return None\n\n    async def query_graphRAG(\n          question: Annotated[str, 'Query string containing information that you want from RAG search']\n                          ) -\u003E str:\n        if LOCAL_SEARCH:\n            result = run_local_search(INPUT_DIR, ROOT_DIR, COMMUNITY ,RESPONSE_TYPE, question)\n        else:\n            result = run_global_search(INPUT_DIR, ROOT_DIR, COMMUNITY ,RESPONSE_TYPE, question)\n        await cl.Message(content=result).send()\n        return result\n\n    for caller in [retriever]:\n        d_retrieve_content = caller.register_for_llm(\n            description=\"retrieve content for code generation and question answering.\", api_style=\"function\"\n        )(query_graphRAG)\n\n    for agents in [user_proxy, retriever]:\n        agents.register_for_execution()(d_retrieve_content)\n\n    groupchat = autogen.GroupChat(\n        agents=[user_proxy, retriever],\n        messages=[],\n        max_round=MAX_ITER,\n        speaker_selection_method=state_transition,\n        allow_repeat_speaker=True,\n    )\n    manager = autogen.GroupChatManager(groupchat=groupchat,\n                                       llm_config=llm_config_autogen, \n                                       is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n                                       code_execution_config=False,\n                                       )    \n\n# -------------------- Conversation Logic. Edit to change your first message based on the Task you want to get done. ----------------------------- # \n    if len(groupchat.messages) == 0: \n      await cl.make_async(user_proxy.initiate_chat)( manager, message=CONTEXT, )\n    elif len(groupchat.messages) \u003C MAX_ITER:\n      await cl.make_async(user_proxy.send)( manager, message=CONTEXT, )\n    elif len(groupchat.messages) == MAX_ITER:  \n      await cl.make_async(user_proxy.send)( manager, message=\"exit\", )","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_63":{"__typename":"Paragraph","id":"55921d74a83c_63","name":"ab32","type":"P","href":null,"layout":null,"metadata":null,"text":"For this application, we only need two agents. You can add\u002Fmodify agents and configure the “state_transition” function to orchestrate speaker selection in conversations for more complex workflows.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_64":{"__typename":"Paragraph","id":"55921d74a83c_64","name":"f471","type":"H3","href":null,"layout":null,"metadata":null,"text":"Final Thoughts","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_65":{"__typename":"Paragraph","id":"55921d74a83c_65","name":"46ec","type":"P","href":null,"layout":null,"metadata":null,"text":"This is my first venture into AI agents, LLMs, and RAGs, and I dove straight into creating this implementation over the past few weeks, bypassing many basics. While this implementation is imperfect, it is an excellent template for developing more complex applications. It lays a solid foundation for integrating multiple functions and coding agents and should enable you to build sophisticated workflows, customize agent interactions, and enhance functionality as needed.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_66":{"__typename":"Paragraph","id":"55921d74a83c_66","name":"87b3","type":"BQ","href":null,"layout":null,"metadata":null,"text":"About Me: I am a lead modeling engineer at Eaton Research Labs, Southfield, MI, USA. I explore, develop tools, and write about things at the intersection of computational mechanics, material science, engineering, language models, and generative AI.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":8,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_67":{"__typename":"Paragraph","id":"55921d74a83c_67","name":"de72","type":"P","href":null,"layout":null,"metadata":null,"text":"If you want to stay updated, follow me on my socials below.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_68":{"__typename":"Paragraph","id":"55921d74a83c_68","name":"d58e","type":"P","href":null,"layout":null,"metadata":null,"text":"Socials: LinkedIn, GitHub, Source Code","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":9,"end":17,"href":"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fkarthik-rajan-venkatesan\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":19,"end":25,"href":"https:\u002F\u002Fgithub.com\u002Fkarthik-codex","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":27,"end":38,"href":"https:\u002F\u002Fgithub.com\u002Fkarthik-codex\u002Fautogen_graphRAG.git","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_69":{"__typename":"Paragraph","id":"55921d74a83c_69","name":"db00","type":"H3","href":null,"layout":null,"metadata":null,"text":"Some Useful References","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_70":{"__typename":"Paragraph","id":"55921d74a83c_70","name":"a657","type":"OLI","href":null,"layout":null,"metadata":null,"text":"https:\u002F\u002Fmedium.com\u002F@datadrifters\u002Fautogen-litellm-and-open-source-llms-c4c6bc8fa9c5","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":82,"href":"https:\u002F\u002Fmedium.com\u002F@datadrifters\u002Fautogen-litellm-and-open-source-llms-c4c6bc8fa9c5","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_71":{"__typename":"Paragraph","id":"55921d74a83c_71","name":"4b49","type":"OLI","href":null,"layout":null,"metadata":null,"text":"https:\u002F\u002Fmedium.com\u002F@rajib76.gcp\u002Fmemory-default-compute-fallback-5ff4287d47e6","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":76,"href":"https:\u002F\u002Fmedium.com\u002F@rajib76.gcp\u002Fmemory-default-compute-fallback-5ff4287d47e6","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_72":{"__typename":"Paragraph","id":"55921d74a83c_72","name":"29b7","type":"OLI","href":null,"layout":null,"metadata":null,"text":"https:\u002F\u002Fmedium.com\u002Fgenerative-ai\u002Fgraphrag-the-rag-approach-by-microsoft-e1abc7eb9fba","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":84,"href":"https:\u002F\u002Fmedium.com\u002Fgenerative-ai\u002Fgraphrag-the-rag-approach-by-microsoft-e1abc7eb9fba","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_73":{"__typename":"Paragraph","id":"55921d74a83c_73","name":"a49c","type":"OLI","href":null,"layout":null,"metadata":null,"text":"https:\u002F\u002Fdocs.chainlit.io\u002Fget-started\u002Foverview","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":45,"href":"https:\u002F\u002Fdocs.chainlit.io\u002Fget-started\u002Foverview","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:55921d74a83c_74":{"__typename":"Paragraph","id":"55921d74a83c_74","name":"ff7a","type":"OLI","href":null,"layout":null,"metadata":null,"text":"https:\u002F\u002Fmedium.com\u002F@antoineross\u002Fautogen-web-application-using-chainlit-8c5ebf5a4e75","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":83,"href":"https:\u002F\u002Fmedium.com\u002F@antoineross\u002Fautogen-web-application-using-chainlit-8c5ebf5a4e75","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"CollectionViewerEdge:collectionId:3fe99b2acc4-viewerId:72a203aea390":{"__typename":"CollectionViewerEdge","id":"collectionId:3fe99b2acc4-viewerId:72a203aea390","isEditor":false,"isMuting":false},"UserViewerEdge:userId:3f1cdf6632b8-viewerId:72a203aea390":{"__typename":"UserViewerEdge","id":"userId:3f1cdf6632b8-viewerId:72a203aea390","isMuting":false},"ImageMetadata:":{"__typename":"ImageMetadata","id":"","originalWidth":0,"originalHeight":0},"PostViewerEdge:postId:61ad3759f06f-viewerId:72a203aea390":{"__typename":"PostViewerEdge","shouldIndexPostForExternalSearch":true,"id":"postId:61ad3759f06f-viewerId:72a203aea390"},"Tag:autogen":{"__typename":"Tag","id":"autogen","displayTitle":"Autogen","normalizedTagSlug":"autogen"},"Tag:ollama":{"__typename":"Tag","id":"ollama","displayTitle":"Ollama","normalizedTagSlug":"ollama"},"Tag:chainlit":{"__typename":"Tag","id":"chainlit","displayTitle":"Chainlit","normalizedTagSlug":"chainlit"},"Tag:rags":{"__typename":"Tag","id":"rags","displayTitle":"Rags","normalizedTagSlug":"rags"},"Tag:graphrag":{"__typename":"Tag","id":"graphrag","displayTitle":"Graphrag","normalizedTagSlug":"graphrag"},"Post:61ad3759f06f":{"__typename":"Post","id":"61ad3759f06f","collection":{"__ref":"Collection:3fe99b2acc4"},"content({\"postMeteringOptions\":{\"referrer\":\"\"}})":{"__typename":"PostContent","isLockedPreviewOnly":false,"bodyModel":{"__typename":"RichText","sections":[{"__typename":"Section","name":"21e6","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"b304","startIndex":16,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"db62","startIndex":64,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}],"paragraphs":[{"__ref":"Paragraph:55921d74a83c_0"},{"__ref":"Paragraph:55921d74a83c_1"},{"__ref":"Paragraph:55921d74a83c_2"},{"__ref":"Paragraph:55921d74a83c_3"},{"__ref":"Paragraph:55921d74a83c_4"},{"__ref":"Paragraph:55921d74a83c_5"},{"__ref":"Paragraph:55921d74a83c_6"},{"__ref":"Paragraph:55921d74a83c_7"},{"__ref":"Paragraph:55921d74a83c_8"},{"__ref":"Paragraph:55921d74a83c_9"},{"__ref":"Paragraph:55921d74a83c_10"},{"__ref":"Paragraph:55921d74a83c_11"},{"__ref":"Paragraph:55921d74a83c_12"},{"__ref":"Paragraph:55921d74a83c_13"},{"__ref":"Paragraph:55921d74a83c_14"},{"__ref":"Paragraph:55921d74a83c_15"},{"__ref":"Paragraph:55921d74a83c_16"},{"__ref":"Paragraph:55921d74a83c_17"},{"__ref":"Paragraph:55921d74a83c_18"},{"__ref":"Paragraph:55921d74a83c_19"},{"__ref":"Paragraph:55921d74a83c_20"},{"__ref":"Paragraph:55921d74a83c_21"},{"__ref":"Paragraph:55921d74a83c_22"},{"__ref":"Paragraph:55921d74a83c_23"},{"__ref":"Paragraph:55921d74a83c_24"},{"__ref":"Paragraph:55921d74a83c_25"},{"__ref":"Paragraph:55921d74a83c_26"},{"__ref":"Paragraph:55921d74a83c_27"},{"__ref":"Paragraph:55921d74a83c_28"},{"__ref":"Paragraph:55921d74a83c_29"},{"__ref":"Paragraph:55921d74a83c_30"},{"__ref":"Paragraph:55921d74a83c_31"},{"__ref":"Paragraph:55921d74a83c_32"},{"__ref":"Paragraph:55921d74a83c_33"},{"__ref":"Paragraph:55921d74a83c_34"},{"__ref":"Paragraph:55921d74a83c_35"},{"__ref":"Paragraph:55921d74a83c_36"},{"__ref":"Paragraph:55921d74a83c_37"},{"__ref":"Paragraph:55921d74a83c_38"},{"__ref":"Paragraph:55921d74a83c_39"},{"__ref":"Paragraph:55921d74a83c_40"},{"__ref":"Paragraph:55921d74a83c_41"},{"__ref":"Paragraph:55921d74a83c_42"},{"__ref":"Paragraph:55921d74a83c_43"},{"__ref":"Paragraph:55921d74a83c_44"},{"__ref":"Paragraph:55921d74a83c_45"},{"__ref":"Paragraph:55921d74a83c_46"},{"__ref":"Paragraph:55921d74a83c_47"},{"__ref":"Paragraph:55921d74a83c_48"},{"__ref":"Paragraph:55921d74a83c_49"},{"__ref":"Paragraph:55921d74a83c_50"},{"__ref":"Paragraph:55921d74a83c_51"},{"__ref":"Paragraph:55921d74a83c_52"},{"__ref":"Paragraph:55921d74a83c_53"},{"__ref":"Paragraph:55921d74a83c_54"},{"__ref":"Paragraph:55921d74a83c_55"},{"__ref":"Paragraph:55921d74a83c_56"},{"__ref":"Paragraph:55921d74a83c_57"},{"__ref":"Paragraph:55921d74a83c_58"},{"__ref":"Paragraph:55921d74a83c_59"},{"__ref":"Paragraph:55921d74a83c_60"},{"__ref":"Paragraph:55921d74a83c_61"},{"__ref":"Paragraph:55921d74a83c_62"},{"__ref":"Paragraph:55921d74a83c_63"},{"__ref":"Paragraph:55921d74a83c_64"},{"__ref":"Paragraph:55921d74a83c_65"},{"__ref":"Paragraph:55921d74a83c_66"},{"__ref":"Paragraph:55921d74a83c_67"},{"__ref":"Paragraph:55921d74a83c_68"},{"__ref":"Paragraph:55921d74a83c_69"},{"__ref":"Paragraph:55921d74a83c_70"},{"__ref":"Paragraph:55921d74a83c_71"},{"__ref":"Paragraph:55921d74a83c_72"},{"__ref":"Paragraph:55921d74a83c_73"},{"__ref":"Paragraph:55921d74a83c_74"}]},"validatedShareKey":"","shareKeyCreator":null},"creator":{"__ref":"User:3f1cdf6632b8"},"inResponseToEntityType":null,"isLocked":false,"isMarkedPaywallOnly":false,"lockedSource":"LOCKED_POST_SOURCE_NONE","mediumUrl":"https:\u002F\u002Fai.gopubby.com\u002Fmicrosofts-graphrag-autogen-ollama-chainlit-fully-local-free-multi-agent-rag-superbot-61ad3759f06f","primaryTopic":null,"topics":[{"__typename":"Topic","slug":"machine-learning"},{"__typename":"Topic","slug":"data-science"},{"__typename":"Topic","slug":"programming"}],"isLimitedState":false,"isPublished":true,"allowResponses":true,"latestPublishedVersion":"55921d74a83c","visibility":"PUBLIC","responsesLocked":false,"postResponses":{"__typename":"PostResponses","count":17},"responseDistribution":"NOT_DISTRIBUTED","clapCount":2062,"title":"Microsoft’s GraphRAG + AutoGen + Ollama + Chainlit = Fully Local & Free Multi-Agent RAG Superbot","isSeries":false,"sequence":null,"uniqueSlug":"microsofts-graphrag-autogen-ollama-chainlit-fully-local-free-multi-agent-rag-superbot-61ad3759f06f","socialTitle":"","socialDek":"","canonicalUrl":"","metaDescription":"","latestPublishedAt":1723158349790,"readingTime":10.39811320754717,"previewContent":{"__typename":"PreviewContent","subtitle":"This superbot app integrates GraphRAG with AutoGen agents, powered by local LLMs from Ollama, for free & offline embedding & inference."},"previewImage":{"__ref":"ImageMetadata:1*2ePkbvke5ObCuJu-g2pzsg.png"},"isShortform":false,"seoTitle":"Local Multi-Agent  RAG Superbot using GraphRAG, AutoGen, Ollama, and Chainlit. | by Karthik Rajan","firstPublishedAt":1721015934326,"updatedAt":1732133089158,"shortformType":"SHORTFORM_TYPE_LINK","seoDescription":"By combining GraphRAG’s RAG capabilities with AutoGen’s conversational and task-oriented functions, you can create a superbot that efficiently handles detailed queries, multi-page report generation, and data analysis. The best part? This app runs locally on a consumer-grade PC or laptop with decent GPUs, ensuring data privacy. ","viewerEdge":{"__ref":"PostViewerEdge:postId:61ad3759f06f-viewerId:72a203aea390"},"isSuspended":false,"license":"ALL_RIGHTS_RESERVED","tags":[{"__ref":"Tag:autogen"},{"__ref":"Tag:ollama"},{"__ref":"Tag:chainlit"},{"__ref":"Tag:rags"},{"__ref":"Tag:graphrag"}],"isFeaturedInPublishedPublication":false,"isNewsletter":false,"statusForCollection":"APPROVED","pendingCollection":null,"detectedLanguage":"en","wordCount":2570,"layerCake":6}}</script><script src="https://cdn-client.medium.com/lite/static/js/manifest.75cc6012.js"></script><script src="https://cdn-client.medium.com/lite/static/js/9865.1496d74a.js"></script><script src="https://cdn-client.medium.com/lite/static/js/main.20d2d781.js"></script><script src="https://cdn-client.medium.com/lite/static/js/instrumentation.5bef8967.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/reporting.ff22a7a5.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9120.5df29668.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5049.d1ead72d.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4505.6dfaf853.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6618.db187378.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9380.fb176dee.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2707.4313e528.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9977.933c1c9a.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8599.68bc318b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/3045.1cc3d8cb.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6349.3329b100.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2648.26563adf.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8393.67b7130d.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6428.7d30b23c.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6199.c727247b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5642.7d9f7f3d.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6546.e46b7f66.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6834.8aa8d357.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4492.0c3e1a1d.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2571.6814b962.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/839.1c286b32.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6128.f8800a13.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2135.780af2f8.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7975.60bcefe8.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/144.7485bbe6.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5240.6281357f.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8819.c627c2bf.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8204.d0637ed0.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/PostPage.MainContent.954c2678.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8414.0d800846.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/3974.8d3e0217.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2527.18a8996d.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/PostResponsesContent.e1e580cb.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/responses.editor.e89462cb.chunk.js"></script><script>window.main();</script></body></html>